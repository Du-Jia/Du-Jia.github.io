<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>pytorch/PyTorch-Learn</title>
      <link href="/2024/08/25/pytorch/PyTorch-Learn/"/>
      <url>/2024/08/25/pytorch/PyTorch-Learn/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2024/08/25/hello-world/"/>
      <url>/2024/08/25/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre class=" language-lang-bash"><code class="language-lang-bash">$ hexo new "My New Post"</code></pre><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre class=" language-lang-bash"><code class="language-lang-bash">$ hexo server</code></pre><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre class=" language-lang-bash"><code class="language-lang-bash">$ hexo generate</code></pre><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre class=" language-lang-bash"><code class="language-lang-bash">$ hexo deploy</code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>PromptBERT: Improving BERT Sentence Embeddings with Prompt</title>
      <link href="/2021/12/10/Prompt/PromptBERT%20Improving%20BERT%20Sentence%20Embeddings/"/>
      <url>/2021/12/10/Prompt/PromptBERT%20Improving%20BERT%20Sentence%20Embeddings/</url>
      
        <content type="html"><![CDATA[<h1 id="PromptBERT-Improving-BERT-Sentence-Embeddings-with-Prompt"><a href="#PromptBERT-Improving-BERT-Sentence-Embeddings-with-Prompt" class="headerlink" title="PromptBERT: Improving BERT Sentence Embeddings with Prompt"></a>PromptBERT: Improving BERT Sentence Embeddings with Prompt</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>本文认为，原始BERT模型在句子语义相似度任务上表现不佳的原因：</p><ol><li>静态token嵌入偏差 (static token embeddings biases)</li><li>无效的BERT层 (ineffective BERT layers)</li></ol><p>而不是因为BERT生成的句子表示的余弦相似度高</p><p><strong>本文的工作</strong>：</p><ol><li>提出了一个基于prompt的<strong>句子嵌入</strong>方法（将sentence embeddings task重构为fill-in-the-black 问题）<ol><li>减少了静态token嵌入偏差</li><li>让原始的BERT更有效</li></ol></li><li>讨论了两种Prompt表示方法以及三种prompt搜索方法</li><li>通过template denoising技术提出了一个新的无监督训练目标</li></ol><h2 id="Instruction"><a href="#Instruction" class="headerlink" title="Instruction"></a>Instruction</h2><p> 1️⃣近年来，在句子表示上，基于BERT的SimCSE等模型有出彩的表现，但是原始的BERT表现很差，甚至不如传统Word Embedding方法GloVe。</p><p>2️⃣有一些研究认为BERT在句子语义相似任务上表现差是因为各向异性(<strong>anisotropy)。</strong>但是比较静态的BERT词向量与最后一层词向量平均池化后的表现(static &gt; last layer)显示，anisotropy不是主要原因。</p><p>3️⃣本文发现，原始的BERT层实际上损害了句子表示的质量。我们发现，分布的偏差不仅和token的频率有关，还对WordPiece中的subword敏感。【通过移除这种高频的subwords或者punctuation，并取其他token的embeddings作为sentence embeddings甚至可以超过BERT-flow和BERT-whitening的表现】</p><p>手动移除偏差token需要大量人力，本文受到prompt方式的启发，提出了一种基于prompt，用模版从BERT中获取句子表示的方法。</p><p>本文提出的方法也可以用于微调。作者发现，通过不同的模版，prompt可以提供更好的生成positive pairs的方法。</p><p>最后，作者提出了一种基于prompt的对比学习方法，这种方法通过template denoising在无监督的设置下利用BERT的能力，能够显著缩短有监督和无监督的表现差距。</p><h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><p>略</p><h2 id="Rethinking-the-Sentence-Embeddings-of-Original-BERT"><a href="#Rethinking-the-Sentence-Embeddings-of-Original-BERT" class="headerlink" title="Rethinking the Sentence Embeddings of Original BERT"></a>Rethinking the Sentence Embeddings of Original BERT</h2><p>作者认为，原始BERT在句子相似度任务上表现不佳的原因是：</p><ul><li>ineffective BERT layers</li><li>static token embeddings biases</li></ul><p>作者通过自己观察到的两种现象验证自己的观点：</p><ol><li><p><strong>Original BERT layers fail to improve the performance</strong>.</p><ul><li><p>衡量句子表示各向异性的方法：</p><p>  $n$为句子的数量，$M$为sentence encoder，$s$为句子</p><p>  <a href="https://imgtu.com/i/o76b4I" target="_blank" rel="noopener"><img src="https://s4.ax1x.com/2021/12/11/o76b4I.png" alt="o76b4I.png"></a></p></li><li><p>比较static embeddings和last layer的平均池化（句子表示）</p><p>  经过BERT的层之后，句子之间的Correlation下降了。</p><p>  各项异性和句子表示的表现没有显示出关联性。</p><p>  （疑问：这里的句子编码是Cross-Encoder还是Bi-Encoder？）</p><p>  <a href="https://imgtu.com/i/o76W36" target="_blank" rel="noopener"><img src="https://s4.ax1x.com/2021/12/11/o76W36.png" alt="o76W36.png"></a></p></li></ul></li><li><p><strong>Embeddings biases harms the sentence embeddings performance</strong>.</p><p> 没理清楚逻辑，之后补</p><ul><li>token embeddings受到token频率(token frequency)、子词(subwords in WordPiece)的影响</li><li>token embeddings 分类：<ul><li>小写开头(lower begin-word tokens)</li><li>大写开头(uppercase begin-word tokens) 【bert-cased】</li><li>子词(subword tokens)</li></ul></li><li><p>下图展示了token frequency，subword对token embeddings分布的影响</p><p>  <a href="https://imgtu.com/i/o765uD" target="_blank" rel="noopener"><img src="https://s4.ax1x.com/2021/12/11/o765uD.png" alt="o765uD.png"></a></p><p>  移除high frequency token，部分sub word， case等就能带来提升</p><p>  <a href="https://imgtu.com/i/o76fgK" target="_blank" rel="noopener"><img src="https://s4.ax1x.com/2021/12/11/o76fgK.png" alt="o76fgK.png"></a></p></li></ul></li></ol><h2 id="Prompt-Based-Sentence-Embeddings"><a href="#Prompt-Based-Sentence-Embeddings" class="headerlink" title="Prompt Based Sentence Embeddings"></a>Prompt Based Sentence Embeddings</h2><p>核心问题：</p><ol><li>怎样用prompt表示句子</li><li>对于sentence embeddings，怎样找到更好的prompt</li></ol><ul><li><p><strong>represent sentence with the prompt</strong></p><p>  使用模版：<strong>“[X] means [MASK]”</strong></p><p>  两种方法：</p><ol><li><p>直接用[MASK]对应的token embedding作为sentence embedding</p><p> $h=h_{[MASK]}$</p></li><li><p>用[MASK]对应的token embedding，用一个MLM分类器选择最好的top-k个token</p><p> $h=\dfrac{\sum_{v\in V_{top-k}}W_vP([MASK]=v|h_{[MASK]})}{\sum_{v\in V_{top-k}}P([MASK]=v|h_{[MASK]})}$</p><p> 缺点：</p><ul><li>因为使用静态token嵌入取平均得到句子表示，仍然会受到偏差(biases)的影响</li><li>在下游任务上不容易fine-tune</li></ul><p>作者最终采用了第一种方式</p></li></ol></li><li><p><strong>prompt search</strong></p><p>  作者讨论了3种方法用于搜索模板：</p><ol><li><p>手工</p><p> 将模版分成两部分：</p><ul><li>关系token：用于句子和「MASK」之间</li><li><p>前缀token：用于修饰句子（「X」前）</p><p>用贪心的方法搜索</p><p><a href="https://imgtu.com/i/o76R9x" target="_blank" rel="noopener"><img src="https://s4.ax1x.com/2021/12/11/o76R9x.png" alt="o76R9x.png"></a></p></li></ul></li><li><p>基于T5生成</p><p> 用T5生成的最好模版：</p><p> <strong>“Also called [MASK]. [X]”.</strong></p><p> 效果比手工生成的差</p></li><li><p>OptiPrompt (Factual probing is [mask]: Learning vs. learning to recall)</p><p> 用连续模版代替离散模版，在STS-B上能将效果提高到80.90</p></li></ol></li><li><p><strong>prompt based contrastive learning with template denoising</strong></p><p>  常用的方法：dropout，adversarial attack, token shuffling, cutoff and dropout in the input token embeddings</p><p>  作者的方法：</p><p>  <strong>用不同的模版对同一个句子生成句子表示，取为positive pairs。</strong></p><p>  方法描述：</p><p>  给定句子$x_i$</p><ol><li>用一个模版计算对应的句子表示$h_i$</li><li>计算模版偏差$\hat{h_i}$：直接送入一个模版和相同的template ids</li><li>用$h_i$$-\hat{h_i}$作为降噪后的句子表示</li><li><p>用$h_i’,\hat{h_i’}$表示用另一个模版生成的句子表示</p><p> 优化下述训练目标：</p><p> $l_i=-log\dfrac{e^{cos(h_i-\hat{h_i}, h_i’-\hat{h_i’})/t}}{\sum_{j=1}^{N} e^{cos(h_i-\hat{h_i},h_j’-\hat{h_j’})/t}}$</p></li></ol></li></ul><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><ul><li>数据集<ul><li>STS</li><li>STS-B</li><li>SICKR</li></ul></li><li>Baselines<ul><li>non fine-tuned:<ul><li>GloVe</li><li>BERT-flow</li><li>BERT-whitening</li></ul></li><li>fine-tuned<ul><li>IS-BERT</li><li>InferSent</li><li>Universal Sentence Encoder</li><li>SBERT</li><li>SimCSE</li><li>ConSERT</li></ul></li></ul></li><li>Implementation Details</li><li><p>Non fine-tuned</p><p>  <a href="https://imgtu.com/i/o76g41" target="_blank" rel="noopener"><img src="https://s4.ax1x.com/2021/12/11/o76g41.png" alt="o76g41.png"></a></p></li><li><p>fine-tuned</p><p>  <a href="https://imgtu.com/i/o76oHH" target="_blank" rel="noopener"><img src="https://s4.ax1x.com/2021/12/11/o76oHH.png" alt="o76oHH.png"></a></p></li><li><p>ablation</p><p>  <a href="https://imgtu.com/i/o76IDe" target="_blank" rel="noopener"><img src="https://s4.ax1x.com/2021/12/11/o76IDe.png" alt="o76IDe.png"></a></p></li></ul><h2 id="Discussion"><a href="#Discussion" class="headerlink" title="Discussion"></a>Discussion</h2><h3 id="Template-Denoising"><a href="#Template-Denoising" class="headerlink" title="Template Denoising"></a>Template Denoising</h3><p>该方法可以移除无关的词汇，例如nothing, no ,yes</p><p>只在对比训练目标函数中使用了该方法，可以帮助消除偏差</p><p><a href="https://imgtu.com/i/o767Ed" target="_blank" rel="noopener"><img src="https://s4.ax1x.com/2021/12/11/o767Ed.png" alt="o767Ed.png"></a></p><h3 id="Stability-in-Unsupervised-Contrastive-Learning"><a href="#Stability-in-Unsupervised-Contrastive-Learning" class="headerlink" title="Stability in Unsupervised Contrastive Learning"></a>Stability in Unsupervised Contrastive Learning</h3><p>用10个不同的随机种子训练模型，与SimCSE对比，PromptBERT的最好和最坏结果差值只有0.53%，SimCSE的差值为3.14%</p><p><a href="https://imgtu.com/i/o76HUA" target="_blank" rel="noopener"><img src="https://s4.ax1x.com/2021/12/11/o76HUA.png" alt="o76HUA.png"></a></p>]]></content>
      
      
      <categories>
          
          <category> 论文阅读笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> 笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PyTorch-SGD-求函数最优值</title>
      <link href="/2021/05/07/pytorch/PyTorch-SGD-%E6%B1%82%E5%87%BD%E6%95%B0%E6%9C%80%E4%BC%98%E5%80%BC/"/>
      <url>/2021/05/07/pytorch/PyTorch-SGD-%E6%B1%82%E5%87%BD%E6%95%B0%E6%9C%80%E4%BC%98%E5%80%BC/</url>
      
        <content type="html"><![CDATA[<h1 id="面试遇到的一个基础问题：使用PyTorch求一个函数的最优值"><a href="#面试遇到的一个基础问题：使用PyTorch求一个函数的最优值" class="headerlink" title="面试遇到的一个基础问题：使用PyTorch求一个函数的最优值"></a>面试遇到的一个基础问题：使用PyTorch求一个函数的最优值</h1><p>代码如下：</p><pre class=" language-lang-(python3)"><code class="language-lang-(python3)">import torch.optim as optimimport torch# 参数为x，(1,1)x = torch.randn([1],requires_grad=True)def function(x):    return x**2 - 2*x + 1optimizer = optim.SGD([x], lr=0.01, momentum=0)for step in range(11):    y = torch.cos(x)    optimizer.zero_grad()    y.backward()    optimizer.step()   #求一次SDG    print('step {}: x={},y={}'.format(step, x[0], y[0]))</code></pre><p>这里将<code>x</code>作为需要优化的参数，每次梯度下降更新<code>x</code>的值，直到目标函数的值达到最小</p><p>这里<code>y</code>即是目标函数</p>]]></content>
      
      
      
        <tags>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>About Me</title>
      <link href="/2020/12/29/about/me/"/>
      <url>/2020/12/29/about/me/</url>
      
        <content type="html"><![CDATA[<h1 id="About-Me"><a href="#About-Me" class="headerlink" title="About Me"></a>About Me</h1><p><a href="https://imgchr.com/i/rHH6sI" target="_blank" rel="noopener"><img src="https://s3.ax1x.com/2020/12/29/rHH6sI.jpg" alt="图片来自网络"></a></p><h2 id="学习经历"><a href="#学习经历" class="headerlink" title="学习经历"></a>学习经历</h2><div class="table-container"><table><thead><tr><th style="text-align:center">学习阶段</th><th style="text-align:center">学校</th><th style="text-align:center">专业</th><th style="text-align:center">研究方向</th></tr></thead><tbody><tr><td style="text-align:center">研究生</td><td style="text-align:center">北京邮电大学</td><td style="text-align:center">计算机技术</td><td style="text-align:center">自然语言处理</td></tr><tr><td style="text-align:center">本科</td><td style="text-align:center">北京信息科技大学</td><td style="text-align:center">计算机科学与技术</td><td style="text-align:center">自然语言处理</td></tr></tbody></table></div><h2 id="工作经历"><a href="#工作经历" class="headerlink" title="工作经历"></a>工作经历</h2><ul><li>科大讯飞，初级算法研究工程师（2023.07 - 至今）</li></ul><h2 id="实习经历"><a href="#实习经历" class="headerlink" title="实习经历"></a>实习经历</h2><ul><li>滴滴 - 用户画像 - 日常实习生 - 用户画像（2021.05 - 2021.08）</li><li>度小满 - AI Lab - 日常实习生 - 预训练、文本匹配（2021.09 - 2022.07）</li><li>澜舟科技 - 搜索组 - 暑期实习生 - （2022.07 - 2022.09）</li></ul><h2 id="论文发表"><a href="#论文发表" class="headerlink" title="论文发表"></a>论文发表</h2><ul><li><a href="https://aclanthology.org/2022.findings-emnlp.285/" target="_blank" rel="noopener">IGATE: Instance-Guided Prompt Learning for Few-Shot Text-Matching.Accepted by findings of EMNLP2022.</a></li></ul><p><strong>联系方式：</strong></p><p>E-mail: jia_du@foxmail.com</p><p><strong>P.S.</strong> 如果你发现文章有错误，可以通过邮件或者评论留言的方式向我提出，我会作出修改，感谢！</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>pytorch_basic</title>
      <link href="/2020/09/22/pytorch/pytorch-basic/"/>
      <url>/2020/09/22/pytorch/pytorch-basic/</url>
      
        <content type="html"><![CDATA[<p><a href="https://imgchr.com/i/wOfiUP" target="_blank" rel="noopener"><img src="https://s1.ax1x.com/2020/09/22/wOfiUP.jpg" alt="封面"></a></p><h1 id="pytorch-baisc"><a href="#pytorch-baisc" class="headerlink" title="pytorch baisc"></a>pytorch baisc</h1><p>最近用Pytorch的时候发现对于pytorch的一些细节问题掌握不够充分。包括整个框架的使用流程，Tensor的一些初始化，类型转换的细节也理解的不是很深。为了方便以后能够更从容的使用Pytorch，重新学习以下Pytorch的一些基础内容，并对重点做一下记录。</p><p>目前大致按照官方tutorials的顺序记录，随着学习的深入和对文档的查阅慢慢补充。</p><h2 id="Basic-Tensor"><a href="#Basic-Tensor" class="headerlink" title="Basic - Tensor"></a>Basic - Tensor</h2><p><strong>包</strong>： <code>torch.Tensor</code></p><ol><li><p>Tensor的创建</p><ul><li><p>直接创建，不初始化： <code>torch.empty()</code></p><p>  e.g., <code>torch.empty(5, 3, dtype=torch.long)</code></p><p>  其中内部的参数是tensor的形状</p><p>相应的，有如下的初始化方式，参数均和上述一致：</p></li><li><p>随机初始化: <code>torch.rand()</code></p></li><li>0值初始化:<code>torch.zeros()</code></li><li>列表初始化：<code>torch.tensor(A_List)</code></li><li><p>用已有Tensor初始化：<code>torch.new_ones()</code>(<em>除非指定数据类型，否则和输入的tensor数据类型相同</em>)</p><pre class=" language-lang-(python)"><code class="language-lang-(python)">  x = x.new_ones(5, 3, dtype=torch.double)  # new_* methods take in sizes  print(x)  x = torch.randn_like(x, dtype=torch.float)  # override dtype!  print(x)</code></pre></li></ul></li><li><p>操作</p><ul><li>形状相同可以直接四则运算(<code>+</code>, <code>-</code>, <code>*</code>, <code>/</code>)，这样的运算是对应位置上的元素在运算</li><li>四则运算函数，返回一个运算结果，可以通过out参数将结果输出到给定tensor：</li><li><code>torch.add(x, y)</code></li><li><code>torch.sub(x, y)</code></li><li><code>torch.mul(x, y)</code></li><li><code>torch.div(x, y, out=result)</code></li><li>原地运算，函数名后缀为下划线的函数，运算结果直接在对应Tensor上改变。</li><li><code>y.add_(x)</code></li><li><code>x.copy_(y)</code></li><li><code>x.t_()</code></li></ul></li><li><p>索引</p><ul><li>可以用numpy数组的索引方式来索引tensor</li><li>如果一个tensor只有一个元素，可以使用<code>x.item()</code>获得该元素</li></ul></li><li><p>改变Tensor形状</p><ul><li><p><code>view()</code>：</p><p>  <code>view()</code>函数接受的参数即是需要修改后的形状。</p><p>  <code>x.view(16)</code>: 将x改为长度为16的向量</p><p>  <code>x.view(-1, 8)</code>: -1表示其他不确定的维度，这个维度需要根据第二个参数8计算出来。例如., 此处x原始的形状为(4,4);那么第二个维度的大小为8，可以计算出第一个维度的大小为2，则新的x的形状为：(2,8)</p></li></ul></li><li><p>numpy.ndarray -&gt; tensor:</p><ul><li><p>如何转化：</p><pre class=" language-lang-(python)"><code class="language-lang-(python)">  # converting numpy array to torch tensor  import numpy as np  a = np.ones(5)  b = torch.from_numpy(a)  np.add(a, 1, out=a)  print(a)  print(b)  # tensor to numpy  a = torch.ones(5)  b = a.numpy()</code></pre><p>可以通过<code>tensor_name.numpy()</code>方法将一个Tensor转化为numpy数组。</p><p><strong>通过Tensor转化的Numpy和原始的Tensor共享内存</strong>：</p><p>也就是说，如果你修改得到的Numpy数组，相应的Tensor内容也会变化。（类似指针操作）</p><p>同理，使用numpy数组生成的Tensor和原始numpy数组也有这样的关系。</p></li></ul></li><li><p>使用GPU</p><p> 使用<code>.to()</code>函数将变量转为GPU数据。</p><p> eg.,</p><pre class=" language-lang-（python）"><code class="language-lang-（python）"> # let us run this cell only if CUDA is available # We will use ``torch.device`` objects to move tensors in and out of GPU if torch.cuda.is_available():     device = torch.device("cuda")          # a CUDA device object     y = torch.ones_like(x, device=device)  # directly create a tensor on GPU     x = x.to(device)                       # or just use strings ``.to("cuda")``     z = x + y     print(z)     print(z.to("cpu", torch.double))       # ``.to`` can also change dtype together!</code></pre></li></ol><p>参考<a href="https://pytorch.org/docs/stable/torch.html" target="_blank" rel="noopener">官方文档</a></p><h2 id="Basic-Autograd-自动求导"><a href="#Basic-Autograd-自动求导" class="headerlink" title="Basic - Autograd, 自动求导"></a>Basic - Autograd, 自动求导</h2><p><strong>包</strong>：<code>torch.autograd</code>, <code>torch.Function</code></p><p>Function: <a href="https://pytorch.org/docs/stable/autograd.html#function" target="_blank" rel="noopener">参考文档</a></p><p><code>autograd</code>这个包为所有在张量上的操作提供了自动求导的功能。</p><p>该框架是在运行时定义的，因此反向传播的方式由代码运行的方式定义，每次迭代都可以不同。</p><ol><li><p><code>requires_grad</code>和<code>grad</code>属性</p><ul><li><code>requires_grad</code>属性，<code>boolean</code>类型，<code>True</code>表示需要自动计算梯度</li><li><code>grad</code>：存储自动计算的梯度</li><li><code>requires_grad_()</code>方法可以改变一个<code>Tensor</code>的该属性的值</li></ul></li><li><p>停止追踪历史计算</p><ul><li><p><code>.detach()</code>：避免张量追溯历史记录或者被之后的计算追溯。(不太明白，后续修改补充。原句：<em>To stop a tensor from tracking history, you can call .detach() to detach it from the computation history, and to prevent future computation from being tracked.</em>)</p><p>如果需要使用一个<code>.requires_grad</code>属性为<code>True</code>的变量，但是又不想对这个变量做的操作被追溯（计算梯度），可以使用<code>.detach()</code>方法得到该变量的副本，并且副本的该属性默认为<code>False</code></p></li></ul></li><li><p>避免追踪历史记录或者使用内存：</p><ul><li>将代码块写在<code>with_torch.no_grad():</code>下。该方法下的代码块在执行时将不会自动计算梯度或者更新参数。适合在对已经训练好的模型测试，评估时使用。</li></ul></li><li><p>自动求导过程中最重要的类：<code>Function</code></p><ul><li><code>Tensor</code>和<code>Function</code>是相关联的，二者一起构建了一个非循环图以编码一个完整的计算过程。</li><li><code>.grad_fn</code>：指向了生成这个<code>Tensor</code>的<code>Function</code>。当属性值为<code>None</code>，表示这个<code>Tensor</code>是由用户创建的</li></ul></li><li><p>求导：<code>.backward()</code></p> <font color="red">此处存疑，未能完全理解</font><p> 使用<code>.backward()</code>求导需要注意的问题：</p><ol><li>如果该<code>Tensor</code>是一个标量，不需要特别指定参数</li><li><p>如果该<code>Tensor</code>含有不止一个元素，需要指定一个<code>gradient</code>参数，这个参数是一个形状匹配的<code>Tensor</code></p><p><code>autograd</code>是一个计算向量-雅可比矩阵乘积的引擎</p><p>下面对此详细说明：</p><p>实际上，<code>.backward()</code>是需要参数的。只不过当<code>Tensor</code>是一个标量的时候，默认方法实质上等价于<code>.backward(torch.tensor(1.))</code>, <code>autograd</code>引擎实际上计算的是$1.0*\dfrac{d(o)}{dx}$，其中1.0被理解为$\dfrac{dl}{do}=1.0$</p><p>把<code>.backward()</code>的形参视为$v$，最终计算的值其实是$J^T\dotv$。（为什么不是$v^T\dotJ$, 因为前者得到的是列向量）</p></li></ol></li><li><p>原始教程</p></li></ol><p><a href="https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html" target="_blank" rel="noopener">点击此处查看原始教程</a></p><h2 id="Basic-Neural-Network"><a href="#Basic-Neural-Network" class="headerlink" title="Basic - Neural Network"></a>Basic - Neural Network</h2><p><strong>包</strong>：<code>torch.nn</code></p><p>重点内容:</p><ul><li>定义模型</li><li><code>nn.Module</code></li><li><code>forward(input)</code></li></ul><ol><li><p>模型训练流程</p><ul><li>定义神经网络，定义可学习的参数</li><li>遍历输入数据集</li><li>处理输入</li><li>计算损失值</li><li>计算梯度</li><li>更新权重</li></ul></li><li><p>定义网络</p><ul><li>继承<code>nn.Module</code></li><li>重写<code>__init__()</code></li><li><p>重写<code>forward()</code></p><p>模型的参数</p></li><li><p><code>model.parameters()</code>方法返回模型中的参数</p></li><li><code>model(input)</code>直接调用<code>forward()</code>方法，返回前馈网络的传播。</li><li><p><code>model.zero_grad()</code>，在反向传播前，应该清空上一次迭代的梯度</p><p><strong>重点</strong>：</p></li><li><p><code>torch.nn</code>只支持mini-batches输入。也就是说，整个<code>troch.nn</code>包中的方法都只支持使用mini-batches作为输入，不支持单个样本的输入。例如2为卷积层接收的输入应该是一个4维的张量，这个张量各个维度的意义是：样本数，通道数，高度，宽度。</p></li><li>如果确实需要输入单个样本，可以考虑使用<code>.unsqueeze(0)</code>方法，为样本增加一个维度。</li></ul></li><li><p>计算loss值</p><p> Loss Function接受一个张量对<code>(output, target)</code>作为输入，并且计算输出和目标值的差距。</p><p> 有很多常用的Loss函数，这里不一一写出，详情参考<a href="https://pytorch.org/docs/stable/nn.html#loss-functions" target="_blank" rel="noopener">Loss Function</a></p> <font color="res">关于Loss函数的一些细节问题之后补充</font></li><li><p>反向传播</p><p> 示例：</p><pre class=" language-lang-(python)"><code class="language-lang-(python)"> net.zero_grad() # zeroes the gradient buffers of all parameters print('conv1.bias.grad before backward') print(net.conv1.bias.grad) loss.backward() print('conv1.bias.grad after backward') print(net.conv1.bias.grad)</code></pre></li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CS224N Lecture 8 Notes</title>
      <link href="/2020/08/18/CS224N/CS224N-Lecture-8-Notes/"/>
      <url>/2020/08/18/CS224N/CS224N-Lecture-8-Notes/</url>
      
        <content type="html"><![CDATA[<h1 id="CS224N-Lecture-8-Notes"><a href="#CS224N-Lecture-8-Notes" class="headerlink" title="CS224N Lecture 8 Notes"></a>CS224N Lecture 8 Notes</h1><p>任务: <strong>Neural Machine Translation</strong></p><p>结构: <strong>Seqence-to-Sequence</strong></p><p>技巧: <strong>Attention</strong></p><p>注：本文所用图片为从课程slides中截取</p><p>之前的课程讲到的任务都没有涉及整个句子的输出，Lecture 8的内容就着眼与这个部分。类似的任务有：</p><ol><li>翻译</li><li>对话</li><li>文本摘要</li></ol><p>类似的任务都是需要输入一个句子（或者更长）并且输出一个句子，Sequence-to-Sequence模型适合解决这类问题。</p><h2 id="统计机器翻译-SMT-Statistical-Machine-Translation"><a href="#统计机器翻译-SMT-Statistical-Machine-Translation" class="headerlink" title="统计机器翻译 SMT(Statistical Machine Translation)"></a>统计机器翻译 SMT(Statistical Machine Translation)</h2><hr><h3 id="翻译模型"><a href="#翻译模型" class="headerlink" title="翻译模型"></a>翻译模型</h3><p>给定一种语言的句子$x$ (Sentence)，求出与之对应的可能性最大的另一种语言的句子$y$</p><script type="math/tex; mode=display">argmax_y P(y|x) = argmax_yP(x|y)P(y) \\(Bayes Rule)</script><h3 id="SMT的训练"><a href="#SMT的训练" class="headerlink" title="SMT的训练"></a>SMT的训练</h3><p>通过平行语料训练，相当于一个解码的过程。主要通过统计方式，用启发式搜索算法计算</p><p><img src="https://s1.ax1x.com/2020/08/18/duEXuV.png" alt="罗塞塔石碑">)</p><p>存在以下问题：</p><ol><li><p>对齐问题(alignment)：</p><p> 两种语言的词在句中的位置可能不是一一对应，可能出现无对应，一对多，多对一，多对多（短语对应短语）等情况出现</p><ul><li>多对一<br><a href="https://imgchr.com/i/duZDWd" target="_blank" rel="noopener"><img src="https://s1.ax1x.com/2020/08/18/duZDWd.md.png" alt="多对一"></a></li><li>一个词对应一个短语<br><a href="https://imgchr.com/i/duZsSA" target="_blank" rel="noopener"><img src="https://s1.ax1x.com/2020/08/18/duZsSA.md.png" alt="一对多"></a></li><li>一对多<br><a href="https://imgchr.com/i/duZBJH" target="_blank" rel="noopener"><img src="https://s1.ax1x.com/2020/08/18/duZBJH.md.png" alt="一对多"></a></li></ul></li><li>一词多义</li></ol><h2 id="神经机器翻译-NMT-Nerual-Machine-Translation"><a href="#神经机器翻译-NMT-Nerual-Machine-Translation" class="headerlink" title="神经机器翻译 NMT(Nerual Machine Translation)"></a>神经机器翻译 NMT(Nerual Machine Translation)</h2><hr><h3 id="Seq2Seq模型"><a href="#Seq2Seq模型" class="headerlink" title="Seq2Seq模型"></a>Seq2Seq模型</h3><p>Seq2Seq模型是一种端到端(end-to-end)模型</p><p>由两个循环神经网络组成(RNN)</p><ul><li><p>Encoder:</p><p>  将输入的序列编码为一个固定大小的“上下文向量”</p><p>  Encoder的最后一个时间步输出的隐藏状态$h$就是“上下文向量”$C$，其网络常采用LSTM。</p><p>  Encoder会<strong>逆序</strong>处理输入的序列，这样可能让EEncoder最开始读入的信息对应最后的输出。Decoder的获取的第一个输入可能会提高其生成合适序列的概率。</p></li><li><p>Decoder：</p><p>  将encoder产生的“上下文向量”用于初始化Decoder的隐藏状态。</p><p>  Decoder是一个语言模型（Language Model，LM）。每个时间步生成的内容又作为下一个时间步的输入。当隐藏状态设置好，就可以开始生成序列了。</p><p>  <strong>生成序列</strong>：</p><p>  Decoder的输入是一个代表开始的特殊表示<code>&lt;start&gt;</code>。并且需要在输入的最后加上表示结束的特殊表示<code>&lt;EOS&gt;</code>，同时这个表示也是输出的结束标志。接下来就按照时间步计算输出序列即可。</p></li></ul><p><strong>训练</strong>：</p><p>在Seq2Seq模型中，定义预测序列的交叉熵(cross-entropy loss)作为模型损失，训练目标就是最小化这个损失。</p><h3 id="NMT"><a href="#NMT" class="headerlink" title="NMT"></a>NMT</h3><p>神经机器翻译采用Seq2Seq模型结构，由两个RNN网络构成。一个RNN作为编码器， 一个RNN作为解码器构成。</p><ul><li>编码器(Encoder)：<br>  负责提取输入的句子的信息，并传递给解码器使用。输出的是输入句子的编码信息。</li><li>解码器(Decoder)：<br>  解码器实际上是一个语言模型(Language Model, LM)，作用是通过编码器传来的信息生成目标语言的句子。</li><li><p>注意：</p><p>  解码器起始输入为<code>&lt;start&gt;</code>。解码器每个单元产生的输出会作为下一个单元的输入。直到产生一个<code>&lt;end&gt;</code></p><p>  编码器和解码器之间没有连接，因此输入语句的长度是任意的，输出语句的长度也是任意的，二者的长度并不一致。</p></li></ul><p>网络结构如下：</p><p><a href="https://imgchr.com/i/dumSUS" target="_blank" rel="noopener"><img src="https://s1.ax1x.com/2020/08/18/dumSUS.md.png" alt="NMT网络结构图"></a></p><h3 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h3><p>模型直接计算：</p><script type="math/tex; mode=display">P(y|x)=\sum_{i=1}^TP(y_i|y_1, ..., y_{i-1},x)</script><p>其中$y$为目标句子，$x$为输入的句子。$y_i$是输出的第$i$个单词。上式描述了解码器生成句子$y$的概率。</p><p>训练，最小化损失函数。Decoder每一个时间步的负log损失相加，就是模型的总损失：</p><script type="math/tex; mode=display">J=\dfrac{1}{T}\sum_{i=1}^{T}J_t</script><h3 id="Decoder怎样求出目标句子"><a href="#Decoder怎样求出目标句子" class="headerlink" title="Decoder怎样求出目标句子"></a>Decoder怎样求出目标句子</h3><ul><li>基础方法：贪心编码<ul><li>每次都求出当前评分最高的句子</li><li>缺点：不能回退</li></ul></li><li>暴力方法：搜索所有可能编码<ul><li>每次都搜索所有可能行</li><li>缺点：计算复杂度高，代价昂贵</li></ul></li><li>Beam search decoding<ul><li>每次选出前$k$种可能性最高的词，然后对每种可能作出评分，留下$k$个可能性最高的编码</li><li>停止条件：<ul><li>规定数目的编码终止(生成了<code>&lt;end&gt;</code>)</li><li>达到规定的时间步</li></ul></li></ul></li><li><p>对句子的评分：</p><ol><li><p>上述每种方法，都需要对句子进行评分然后比较，评分公式如下：</p><script type="math/tex; mode=display">Score(y_1,...,y_t) = logP_{LM}(y_1,...,y_t|x)=\sum_{i=1}^t\log{P_{LM}(y_i|y_1,...,y_{i-1},x)}</script></li><li><p>改进：由于句子长度对评分有所影响（句子越长，评分越低）因此对评分做规范化(Normalize)处理</p><script type="math/tex; mode=display">Score(y_1,...,y_t) = \dfrac{1}{t}logP_{LM}(y_1,...,y_t|x)=\dfrac{1}{t}\sum_{i=1}^t\log{P_{LM}(y_i|y_1,...,y_{i-1},x)}</script></li></ol></li></ul><h3 id="改进：双向循环神经网络"><a href="#改进：双向循环神经网络" class="headerlink" title="改进：双向循环神经网络"></a>改进：双向循环神经网络</h3><p>最基本的NMT结构中，编码器和解码器中的信息都是单向流动的，但是在句子中，词和词之间的依赖不一定是单向的，因此，通过在编码器和解码器中增加一个方向的RNN，实现双向的RNN结构，有助于获取词和词之间在两个方向上的依赖信息。</p><p>由于采用了两个方向的RNN，编码层产生的“上下文向量”也就有了两个：一个正向，一个反向（reverse context vector, forward context vector）。用于初始化解码器中的对应方向的RNN网络。</p><p>采用双向循环神经网络的编码器：</p><p><img src="https://s1.ax1x.com/2020/08/18/dKmQld.png" alt="双向循环神经网络编码器"></p><h3 id="NMT的优缺点"><a href="#NMT的优缺点" class="headerlink" title="NMT的优缺点"></a>NMT的优缺点</h3><h4 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h4><ol><li>表现比SMT更好</li><li>只需要一个单独的神经网络就能完成训练</li><li>更少的特征工程</li></ol><h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><ol><li>可解释性差</li><li>很难控制</li></ol><h3 id="NMT的难点"><a href="#NMT的难点" class="headerlink" title="NMT的难点"></a>NMT的难点</h3><ol><li>超出词汇表(out-of-vocabulary)</li><li>领域不匹配(Domin mismatch)</li><li>难以从长文本中获取信息</li><li>一些语言的训练语料很少（NMT需要大量训练语料）</li><li>偏见（训练出的模型在处理一些句子时会出现偏向某一个性别的现象）</li><li>可解释性差</li></ol><h2 id="注意力机制-Attention"><a href="#注意力机制-Attention" class="headerlink" title="注意力机制 (Attention)"></a>注意力机制 (Attention)</h2><hr><ul><li><p>来源：</p><p>  一个输入中的不同部分应该具有不同等级的重要性。</p><p>  例如：</p><p>  <em>input:</em> the ball is on the filed</p><p>  句中的每个单词的重要性是不同的，ball和filed两个单词的重要性明显更高。</p></li><li><p>作用：</p><p>  注意力机制的作用就在于：</p><p>  为解码器的每个时间步都提供了输入句子的全局信息（这个信息经过注意力机制后强调了重点部分），因此在不同的时间步，解码器可以决定哪些输入的内容在当前是重要的。</p></li><li><p>注意力机制图示：</p></li></ul><p><img src="https://s1.ax1x.com/2020/08/18/duELj0.png" alt="注意力机制"></p><ul><li><p>为什么要使用注意力机制：</p><p>  基本的NMT模型中，编码器的最后一个时间步的输出隐状态传送到解码器中作为输入。当句子较长的时候，很难获取到较远时间步的信息。</p><p>  引入注意力机制就是为了捕获原句中的所有信息（重要信息）</p></li><li><p>结构</p><p>  上图为解码器在最后一个时间步使用Attention的图示。</p><p>  注意力机制获取编码器中每个时间步的输出，结合解码器中当前时间步的输出，通过<code>scores</code>函数求出相应的评分，得到其分布情况(Attention Distribution)。而后通过<code>softmax</code>得到注意力层的输出(Attention output)。然后将注意力层的输出和解码器该时间步的隐含状态拼接起来按照和未使用注意力机制时一样的处理方式处理。</p><p>  具体的公式如下：</p><ul><li><p>score函数</p><script type="math/tex; mode=display">e^t = [s_t^Th_1,...,s_t^Th_N]</script></li><li><p>使用softmax函数计算分布：</p><script type="math/tex; mode=display">\alpha^t = softmax(e^t)</script></li><li><p>计算Attention output</p><script type="math/tex; mode=display">a_t=\sum_{i=1}^N\alpha^t_ih_i</script></li><li><p>连接注意力层的输出和解码器的隐含状态：</p><script type="math/tex; mode=display">[a_t;s_t]</script></li><li>其中$e^t$表示attention scores, $\alpha^t$是其分布，$a_t$是注意力层最终的输出，$s_t$是当前解码器时间步输出的隐藏状态。</li></ul></li></ul><h3 id="注意力机制的作用"><a href="#注意力机制的作用" class="headerlink" title="注意力机制的作用"></a>注意力机制的作用</h3><ol><li>提高NMT的表现，打破NMT的瓶颈，并且对梯度消失问题有所帮助</li><li><p>为NMT模型提供了一定的可解释性</p><ul><li>通过注意力层的评分，可以观察到句子中哪些信息比较重要</li><li>网络自学习到了关于不同语言单词对齐(alignment)的信息</li></ul></li></ol><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>第8课主要涉及以下内容：</p><ol><li>Machine Translation的历史：SMT</li><li>基本的Neural Machine Translation（NMT）架构Seq2Seq</li><li>NMT的优缺点及难点</li><li>引入注意力机制解决编码器捕获长序列信息的瓶颈</li></ol><p>PS: 对于Lecture内中提及的其他模型及对NMT结果的评估部分内容未完成，后续逐渐补充</p>]]></content>
      
      
      <categories>
          
          <category> CS224N笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> 笔记 </tag>
            
            <tag> CS224N </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Deep Learning Notes - Week 1</title>
      <link href="/2020/08/05/other/Deep-Learning-Notes-Week-1/"/>
      <url>/2020/08/05/other/Deep-Learning-Notes-Week-1/</url>
      
        <content type="html"><![CDATA[<h1 id="NLP入门计划第一周-笔记"><a href="#NLP入门计划第一周-笔记" class="headerlink" title="NLP入门计划第一周 - 笔记"></a>NLP入门计划第一周 - 笔记</h1><h2 id="内容"><a href="#内容" class="headerlink" title="内容"></a>内容</h2><ol><li>了解感知机/多层感知机(MLP)</li><li>了解<strong>前向传播</strong>，<strong>反向传播</strong>，<strong>微积分</strong>， <strong>偏导</strong>的基本知识</li><li>梯度下降，学习率，梯度裁剪</li><li>了解常用的激活函数，从$tanh$到$gelu$</li><li>了解<strong>loss function</strong>, <strong>cross-entropy</strong>, <strong>平方差</strong></li></ol><h2 id="笔记"><a href="#笔记" class="headerlink" title="笔记"></a>笔记</h2><p>由于之前有过一点深度学习基础，不算是完全从零开始学起，此次做笔记的目的在于加深对深度学习基础知识的了解，特别是补了一下一些数学上的理论知识。经验浅薄，有所疏漏在所难免。</p><h3 id="感知机-Perceptron"><a href="#感知机-Perceptron" class="headerlink" title="感知机(Perceptron)"></a>感知机(Perceptron)</h3><p>感知机用于线性可分数据的二分类任务，基本思想是通过求解能够划分两类数据的直线或者超平面以达到分类的目的。</p><p>按照《统计学习方法》一书的介绍，下面从<strong>模型</strong>，<strong>策略</strong>，<strong>算法</strong>三块来介绍。</p><ol><li><p>模型</p><p> 感知机可以表示为如下形式:</p><script type="math/tex; mode=display">f(x) = sign(wx + b)</script><p> 即一个空间上的超平面。其中$w$为该超平面的斜率，$b$为该超平面的截距。</p><p> 最终学习的目标就是学习到一个能够把数据点分到该朝平面两侧的超平面。</p></li><li><p>学习策略</p><ul><li><p>误分类点：</p><p>  误分类点即被超平面错误分类的点，其$f(x)$值和实际的标签$y$恰好相反</p><p>  误分类点可以提供修正模型的数据</p></li><li><p>损失函数：</p><p>  即误分类点到超平面的总距离，最终的目标就是让该值尽可能小</p><p>  损失函数表示如下：</p><script type="math/tex; mode=display">-\dfrac{1}{\|w\|}\displaystyle\sum_{x_i\in M}|w\cdot{x_i}+{b}|</script></li></ul></li><li><p>训练算法</p><p> 随机梯度下降</p><p> 思想：随机初始化超平面，<strong>每次</strong>随机选择<strong>一个</strong>误分类点使超平面梯度下降，不断重复，直到所有点被正确分类或者损失函数达到一个<strong>阈值</strong>（如果是线性不可分的数据是不可能求出能正确分类的超平面的）</p></li></ol><h3 id="多层感知机-multilayer-perceptron"><a href="#多层感知机-multilayer-perceptron" class="headerlink" title="多层感知机(multilayer perceptron)"></a>多层感知机(multilayer perceptron)</h3><p>$多层感知机(MLP) = 人工神经网络(ANN)$</p><p>单层感知机只能处理线性可分的数据，因此遇到更复杂的数据时，感知机就没有作用了。</p><p>多层感知机的结构如下图所示（这里使用了一张别人博客中的图片，<a href="https://www.cnblogs.com/jaww/p/12302543.html" target="_blank" rel="noopener">点击此处</a>跳转到博客原文）：</p><p><img src="https://img2018.cnblogs.com/i-beta/1473572/202002/1473572-20200213100738646-47348751.png" alt="多层感知机"></p><p>如上图所示，多层感知机其实就是一个全连接的神经网络。上图所示为一个含一层隐含层的网络。这里的$h_1 = w_1x + b$实际上就是一个感知机，多个感知机层叠到一起，构成了一个多层感知机。</p><p>但是，如果隐藏层的单元$h_1, h_2, …, h_n$如果直接输入到下一层，那么该多层感知机实际上仍然只能处理线性可能的数据，也即是说，仍然等价于一个单层的感知机，只不过参数更多更复杂而已。要使其能够处理非线性可分的数据，必须加入激活函数（引入非线性），这时隐含层的单元值有所变化，下面以$h_1$为例：</p><script type="math/tex; mode=display">h_1 = sigmoid(w_1x + b)</script><p>这样的网络，才是一个真正的多层感知机。</p><h3 id="前向传播-forward-propagation"><a href="#前向传播-forward-propagation" class="headerlink" title="前向传播(forward propagation)"></a>前向传播(forward propagation)</h3><p>如前节，按照图中的描述，输入层数据$X$经过计算可以得到隐含层数据$H$，再由隐含层数据经过计算得到输出层的数据$O$，这样一个过程，就是前向传播。</p><h3 id="反向传播-backpropagation"><a href="#反向传播-backpropagation" class="headerlink" title="反向传播(backpropagation)"></a>反向传播(backpropagation)</h3><p>模型的训练过程即是求解<strong>目标函数</strong>最优值的过程。（目标函数即是代价函数，用于衡量训练结果和实际标签的差异）</p><p>反向传播即是通过<strong>梯度下降</strong>方法，从最后一层开始，使用每一层的梯度值逐层更新每一层的参数值，最终不断逼近最优解（可能是局部最优）</p><h3 id="学习率"><a href="#学习率" class="headerlink" title="学习率"></a>学习率</h3><p>每次更新的幅度大小，$0 &lt;= lr &lt;= 1$</p><h3 id="梯度消失和梯度爆炸"><a href="#梯度消失和梯度爆炸" class="headerlink" title="梯度消失和梯度爆炸"></a>梯度消失和梯度爆炸</h3><p>当函数的偏导值接近0，会出现梯度消失（即该参数无法通过梯度进行更新）。梯度消失和激活函数的选取以及网络结构有关。</p><p><strong>梯度爆炸的原因暂时还未理解，后续补坑。</strong></p><h3 id="梯度裁剪"><a href="#梯度裁剪" class="headerlink" title="梯度裁剪"></a>梯度裁剪</h3><p>可以防止梯度爆炸问题发生</p><h3 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h3><p>常用激活函数及其导数</p><ol><li>sigmoid<ul><li>原函数<script type="math/tex; mode=display">sigmoid(x)=\dfrac{1}{1+e^{-x}}</script></li><li>导数<script type="math/tex; mode=display">sigmoid(x)^{'}=sigmoid(x)(1-sigmoid(x))</script></li></ul></li><li>tanh<ul><li>原函数<script type="math/tex; mode=display">tanh(x)=\dfrac{e^x-e^{-x}}{e^x+e^{-x}}=2sigmoid(2x)-1</script></li><li>导数<script type="math/tex; mode=display">tanh(x)^{'}=1-tanh(x)^2</script></li></ul></li><li>ReLU(线性整流单元)<ul><li>原函数<script type="math/tex; mode=display">f(x)=\begin{cases}x &\text{if } x\ge0 \\ 0 &\text{if }x<0\end{cases}</script></li><li>导数<script type="math/tex; mode=display">f(x){'}=\begin{cases}1 &\text{if } x\ge0 \\ 0 &\text{if }x<0\end{cases}</script></li></ul></li><li>leakly ReLU<ul><li>原函数<script type="math/tex; mode=display">f(x)=\begin{cases}x &\text{if } x\ge0 \\ \alpha{x} &\text{if }x<0\end{cases}\left(\alpha=0.1\right)</script></li><li>导数<script type="math/tex; mode=display">f(x){'}=\begin{cases}1 &\text{if } x\ge0 \\ \alpha &\text{if }x<0\end{cases}(\alpha=0.1)</script></li></ul></li><li>ELU (SELU)<ul><li>原函数<script type="math/tex; mode=display">f(x)=\begin{cases}x &\text{if } x\ge0 \\ \alpha\left({e^x-1}\right) &\text{if }x<0\end{cases}\left(\alpha=0.1\right)</script></li><li>导数<script type="math/tex; mode=display">f(x){'}=\begin{cases}1 &\text{if } x\ge0 \\ \alpha{e^x} &\text{if }x<0\end{cases}(\alpha=0.1)</script></li></ul></li><li>GELU<ul><li>原函数<script type="math/tex; mode=display">f(x)=0.5x(1+tanh(\sqrt{\dfrac{2}{\pi}}(x+0.44715x^3)))</script></li><li>导数</li></ul></li><li>softmax<ul><li>原函数<script type="math/tex; mode=display">f(x)=\dfrac{e^{Z_j}}{\sum_{k=1}^{K}{e^{Z_k}}}</script></li><li>导数</li></ul></li><li>Maxout<ul><li>原函数<script type="math/tex; mode=display">f(x)=max(W_1x+b_1, W_2x+b_2)</script></li><li>导数</li></ul></li></ol><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p><em>损失函数是$f(X)$和$Y$的非负实值函数——《统计学习方法》</em></p><ul><li>0-1 损失<script type="math/tex; mode=display">L(f(X),Y)=\begin{cases}1,&Y\not=f(X)\\0, &Y=f(X)\end{cases}</script></li><li>平方损失<script type="math/tex; mode=display">L(f(X),Y)=(Y-f(X))^2</script></li><li>绝对损失<script type="math/tex; mode=display">L(f(X),Y)=|Y-f(x)|</script></li><li>对数损失<script type="math/tex; mode=display">L(Y,P(Y|X))=-\log{P(Y|X)}</script></li><li>均方误差<script type="math/tex; mode=display">MSE=\dfrac{1}{n}(\sum{\hat{y_i}-y_i})^2</script></li></ul><h3 id="交叉熵"><a href="#交叉熵" class="headerlink" title="交叉熵"></a>交叉熵</h3><p>用于衡量预测的概率分布和实际概率分布的差异</p><script type="math/tex; mode=display">Cross-Entropy=-\sum_{i=1}^np(x_i)\log(q(x_i))</script>]]></content>
      
      
      <categories>
          
          <category> NLP入门计划-笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 笔记 </tag>
            
            <tag> 机器学习 </tag>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pandas数据分析</title>
      <link href="/2020/04/04/other/Pandas%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
      <url>/2020/04/04/other/Pandas%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<h1 id="Pandas数据分析"><a href="#Pandas数据分析" class="headerlink" title="Pandas数据分析"></a>Pandas数据分析</h1><p>记录一下Pandas的基本使用技巧，主要针对对csv文件的分析</p><p>先把坑挖好</p>]]></content>
      
      
      
        <tags>
            
            <tag> 笔记 </tag>
            
            <tag> 数据分析 </tag>
            
            <tag> Pandas </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CS224N-Lecture5-notes</title>
      <link href="/2020/04/02/CS224N/CS224N-Lecture5-notes/"/>
      <url>/2020/04/02/CS224N/CS224N-Lecture5-notes/</url>
      
        <content type="html"><![CDATA[<h1 id="CS224N-Assignment-3-Neural-Dependency-Parser"><a href="#CS224N-Assignment-3-Neural-Dependency-Parser" class="headerlink" title="CS224N Assignment 3 Neural Dependency Parser"></a>CS224N Assignment 3 Neural Dependency Parser</h1><h2 id="需要理清楚的细节问题"><a href="#需要理清楚的细节问题" class="headerlink" title="需要理清楚的细节问题"></a>需要理清楚的细节问题</h2><p>网络结构</p><ol><li>输入数据的形式<ul><li>原始输入数据分为3个部分：Word，POS，Label（单词，词性标签，弧的类型）。处理后的输入数据$[x^w, x^t, x^l]$</li><li>其中word是按照一定的规则从数据中抽取的特征，包含18个词（stack，buffer栈顶6个 + stack栈顶2个单词的最左2个，最右2个 + stack栈顶两个元素的最左，最右的最左最右单词，共计4个）</li><li>3个特征都需要Embedding</li></ul></li><li>Embedding是从已经训练好的文件中加载的还是训练过程中同步训练出来的？<ul><li>使用的是预训练好的Embedding矩阵，位置<code>./data/en_cw.txt</code></li></ul></li><li>各层模型对数据类型的具体要求<ul><li><strong>存疑</strong></li></ul></li><li>mini_batch的问题，怎样划分mini_batch,每个batch的数据是同时输入做训练还是batch内部有区别<ul><li>针对本作业：minibatch的划分是为了按照批次处理数据，由于句子的长度不同，需要处理的步数不同，因此每个批次的数据不是按照固定的步骤数生成，而是每批次数据都需要等到该批次数据全部解析完。</li><li>其他：这里用到的参数是batch_size, 那么数据一共能分成<code>dataset_size / batch_szie</code>向下取整个batch</li></ul></li><li>loss值的计算，这里用的是softmax，Pytorch中不同的损失值的计算的输入，输出，对数据类型的要求是什么，形状是什么<ul><li>这里的输出对应transition的3种类型，是一个1*3的向量，每个元素对应该类别的概率。因此最终采用交叉熵计算loss值。</li></ul></li><li>UAS的计算方式。神经网络预测出的结果，最终是通过什么方式评估，量化并得到最终的评分的，标注好的数据集是什么形式<ul><li>Unlabeled attachment score (UAS) = head</li><li>也就是说UAS只需要关注弧的指向是否正确，而不关注弧的标签是否正确。因此，与之对应的：LAS（Labeled attachment score）两者都关注</li></ul></li><li>tqdm进度条的使用方法</li><li>dropout的概率参数是指的保留的神经元数目还是指的丢弃的神经元数目。<ul><li>dropout：训练的时候使用，随机把隐含层的单元设为0(每个批次drop不同的单元)，然后将隐含层和一个常数$r$相乘得到$h_{drop}$。这个常数使得$h_{drop}$的期望和$h$的期望相同</li><li>dropout层的目的是防止过拟合</li></ul></li><li>dropout层是一个单独的层还是仅仅是作用于隐含层上的一个函数<ul><li>是一个单独的层，只用于训练的时候，为了防止过拟合。（因此一般加在全连接层后面）</li><li>drop prop的意义是隐含层的单元有drop prop的概率会被”drop“</li></ul></li><li>Adam算法的思路回顾</li></ol><h2 id="收获"><a href="#收获" class="headerlink" title="收获"></a>收获</h2><ol><li>对CS224N的第5课有了更深入的理解</li><li>实际使用PyTorch搭建了神经网络，对这个问题有所了解了</li><li>更深地理解了从<code>原始数据-&gt;特征抽取-&gt;嵌入-&gt;训练-&gt;验证</code>这个过程</li><li>理解了每次课讲的<code>评估</code>这个部分</li></ol>]]></content>
      
      
      <categories>
          
          <category> CS224N笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> 笔记 </tag>
            
            <tag> CS224N </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>numpy基础</title>
      <link href="/2020/03/13/other/numpy%E5%9F%BA%E7%A1%80/"/>
      <url>/2020/03/13/other/numpy%E5%9F%BA%E7%A1%80/</url>
      
        <content type="html"><![CDATA[  <div tabindex="-1" id="notebook" class="border-box-sizing">    <div class="container" id="notebook-container"><div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt"></div><div class="inner_cell"><div class="text_cell_render border-box-sizing rendered_html"><h1 id="Numpy-学习笔记">Numpy 学习笔记<a class="anchor-link" href="#Numpy-学习笔记">¶</a></h1><h2 id="1.-基础函数">1. 基础函数<a class="anchor-link" href="#1.-基础函数">¶</a></h2></div></div></div><div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt"></div><div class="inner_cell"><div class="text_cell_render border-box-sizing rendered_html"><ul><li>创建</li><li>打印</li><li>基本操作</li><li>常用函数</li><li>下标，切片，迭代</li></ul></div></div></div><div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt"></div><div class="inner_cell"><div class="text_cell_render border-box-sizing rendered_html"><ol><li>numpy array 和 python 序列的差异<ul><li>创建时固定大小（形状），改变大小相当于删除重建</li><li>元素类型相同</li><li>比python内建数据类型处理速度更快</li><li>被许多更高层次的库所使用（如Tensorflow, Pytorch)</li><li>两大基础功能： 向量化(vectorization) ，广播(broadcasting)</li></ul></li><li>为什么numpy效率更高<ul><li>更简洁的代码</li><li>更接近数学符号的表达形式</li></ul></li><li><p>numpy数组： numpy.array</p><ul><li><p>使用Python序列作为参数创建</p><p>可以自动将序列转化为对应维度的numpy数组</p><pre><code>(Python)  # 一维  a = np.array([2,3,4])  # 二维  b = np.array([(1.5,2,3), (4,5,6)])  # array([[1.5, 2. , 3. ],  #      [4. , 5. , 6. ]])</code></pre><p>可以指定元素类型</p><pre><code>(Python)  c = np.array( [ [1,2], [3,4] ], dtype=complex )</code></pre></li><li><p>使用占位符(Placeholder)创建特定形状的numpy数组</p><ul><li><p><code>numpy.zeros(shape, dtype)</code></p><p>用0填充空位</p><p>shape: 需要创建的n维数组的形状，turple类型(元组)</p><p>e.g.,</p><pre><code>(Python)np.zeros((3, 4))# array([[0., 0., 0., 0.],#         [0., 0., 0., 0.],#         [0., 0., 0., 0.]])</code></pre></li><li><p><code>numpy.ones(shape, dtype)</code></p><p>用1填充空位</p></li><li><p><code>numpy.empty(shape, dtype)</code></p><p>随机填充</p></li><li><p>使用<code>zeros_like</code>等函数填充</p></li></ul></li><li><p>使用arange(类似range)</p></li><li>使用linsapce（类似range，但是生成的是浮点数）</li><li>显示：<ul><li>最后的轴横向显示</li><li>倒数第二的轴上下显示</li><li>如有更高维度，也上下显示</li><li>中间元素会被省略，如需强制输出可用：<code>np.set_printoptions(threshold=sys.maxsize)</code></li></ul></li><li>使用reshape更改数组的形状</li></ul></li><li><p>基本运算</p><p>numpy数组的运算方法作用到每一个元素</p><p>作用到每个元素的四则运算：</p><p><code>+</code>, <code>-</code>, <code>*</code>, <code>/</code></p><p>矩阵乘法：</p><p><code>@</code></p></li><li><p>一元操作</p><ul><li>sum(axis)</li><li>max(axis)</li><li>min(axis)</li><li><p>cumsum</p><p>一元操作可以通过<code>axis</code>参数指定操作的方向</p></li></ul></li><li><p>其他操作</p><ul><li>np.sqrt</li><li>np.exp</li><li>np.add</li><li>...</li></ul></li><li><p>下标，切片，迭代</p><ul><li><p>对多维数组的所有元素操作:<code>flat</code>属性</p><pre><code>(Python)  for element in b.flat:      print(element)</code></pre></li></ul></li><li><p>形状操作</p><ul><li>返回新数组，不改动原数组<ul><li><code>ravel</code>： 返回flatten后的数组</li><li><code>reshape(axis1, axis2, ...)</code>：返回形状为:（axaxis1, axis2, ...）的数组</li><li><code>T</code>：返回转置后的数组</li></ul></li><li>修改原始数组:<ul><li><code>resize()</code></li></ul></li></ul></li></ol></div></div></div><div class="cell border-box-sizing code_cell rendered"><div class="input"><div class="prompt input_prompt">In&nbsp;[2]:</div><div class="inner_cell">    <div class="input_area"><div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span></pre></div>    </div></div></div></div><div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt"></div><div class="inner_cell"><div class="text_cell_render border-box-sizing rendered_html"><p>Numpy中的维度被称为轴</p><p>一个一维数组有一个轴，二维数组有两个轴，依此类推。</p></div></div></div><div class="cell border-box-sizing code_cell rendered"><div class="input"><div class="prompt input_prompt">In&nbsp;[3]:</div><div class="inner_cell">    <div class="input_area"><div class=" highlight hl-ipython3"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">15</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span><span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">ndim</span><span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">itemsize</span><span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">size</span><span class="p">)</span><span class="nb">type</span><span class="p">(</span><span class="n">a</span><span class="p">)</span></pre></div>    </div></div></div><div class="output_wrapper"><div class="output"><div class="output_area">    <div class="prompt"></div><div class="output_subarea output_stream output_stdout output_text"><pre>(3, 5) 2 4 15</pre></div></div><div class="output_area">    <div class="prompt output_prompt">Out[3]:</div><div class="output_text output_subarea output_execute_result"><pre>numpy.ndarray</pre></div></div></div></div></div><div class="cell border-box-sizing code_cell rendered"><div class="input"><div class="prompt input_prompt">In&nbsp;[33]:</div><div class="inner_cell">    <div class="input_area"><div class=" highlight hl-ipython3"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="p">[[</span> <span class="mi">0</span><span class="p">,</span>  <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">],</span>       <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">13</span><span class="p">],</span>       <span class="p">[</span><span class="mi">20</span><span class="p">,</span> <span class="mi">21</span><span class="p">,</span> <span class="mi">22</span><span class="p">,</span> <span class="mi">23</span><span class="p">],</span>       <span class="p">[</span><span class="mi">30</span><span class="p">,</span> <span class="mi">31</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">33</span><span class="p">],</span>       <span class="p">[</span><span class="mi">40</span><span class="p">,</span> <span class="mi">41</span><span class="p">,</span> <span class="mi">42</span><span class="p">,</span> <span class="mi">43</span><span class="p">]]</span><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">a</span><span class="p">)</span><span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span> <span class="mi">0</span><span class="p">,</span>  <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">],</span>       <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">12</span><span class="p">],</span>       <span class="p">[</span><span class="mi">20</span><span class="p">,</span> <span class="mi">21</span><span class="p">,</span> <span class="mi">22</span><span class="p">],</span>       <span class="p">[</span><span class="mi">30</span><span class="p">,</span> <span class="mi">31</span><span class="p">,</span> <span class="mi">32</span><span class="p">],</span>       <span class="p">[</span><span class="mi">40</span><span class="p">,</span> <span class="mi">41</span><span class="p">,</span> <span class="mi">42</span><span class="p">]])</span><span class="nb">print</span><span class="p">(</span><span class="s1">'A:'</span><span class="p">,</span><span class="n">a</span><span class="p">)</span><span class="nb">print</span><span class="p">(</span><span class="s1">'B:'</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span><span class="c1"># print('Vstack: ', np.vstack((a,b)))</span><span class="nb">print</span><span class="p">(</span><span class="s1">'HStack:'</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">)))</span><span class="nb">print</span><span class="p">(</span><span class="s1">'ColStack:'</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">((</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">)))</span></pre></div>    </div></div></div><div class="output_wrapper"><div class="output"><div class="output_area">    <div class="prompt"></div><div class="output_subarea output_stream output_stdout output_text"><pre>A: [[ 0  1  2  3] [10 11 12 13] [20 21 22 23] [30 31 32 33] [40 41 42 43]]B: [[ 0  1  2] [10 11 12] [20 21 22] [30 31 32] [40 41 42]]HStack: [[ 0  1  2  3  0  1  2] [10 11 12 13 10 11 12] [20 21 22 23 20 21 22] [30 31 32 33 30 31 32] [40 41 42 43 40 41 42]]ColStack: [[ 0  1  2  3  0  1  2] [10 11 12 13 10 11 12] [20 21 22 23 20 21 22] [30 31 32 33 30 31 32] [40 41 42 43 40 41 42]]</pre></div></div></div></div></div><div class="cell border-box-sizing code_cell rendered"><div class="input"><div class="prompt input_prompt">In&nbsp;[30]:</div><div class="inner_cell">    <div class="input_area"><div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">numpy</span> <span class="k">import</span> <span class="n">newaxis</span></pre></div>    </div></div></div></div><div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt"></div><div class="inner_cell"><div class="text_cell_render border-box-sizing rendered_html"><p>Stack:</p><p>将两个数组按行或者按列拼接起来（堆叠）（形状相同）</p><ul><li>vstack</li><li>hstack</li><li>column_stack</li><li>row_stack</li></ul><p>在任何情况下<code>vstack = row_stack</code></p><p>在1维或者2维情况下：<code>hstack = column_stack</code></p><p>In general, for arrays with more than two dimensions, hstack stacks along their second axes, vstack stacks along their first axes, and concatenate allows for an optional arguments giving the number of the axis along which the concatenation should happen.</p></div></div></div><div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt"></div><div class="inner_cell"><div class="text_cell_render border-box-sizing rendered_html"><p><strong>切分数组为几个更小的数组</strong></p></div></div></div><div class="cell border-box-sizing code_cell rendered"><div class="input"><div class="prompt input_prompt">In&nbsp;[34]:</div><div class="inner_cell">    <div class="input_area"><div class=" highlight hl-ipython3"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">6.</span><span class="p">,</span> <span class="mf">7.</span><span class="p">,</span> <span class="mf">6.</span><span class="p">,</span> <span class="mf">9.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">5.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">6.</span><span class="p">,</span> <span class="mf">8.</span><span class="p">,</span> <span class="mf">5.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">],</span>       <span class="p">[</span><span class="mf">8.</span><span class="p">,</span> <span class="mf">5.</span><span class="p">,</span> <span class="mf">5.</span><span class="p">,</span> <span class="mf">7.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">8.</span><span class="p">,</span> <span class="mf">6.</span><span class="p">,</span> <span class="mf">7.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">8.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">]])</span></pre></div>    </div></div></div></div><div class="cell border-box-sizing code_cell rendered"><div class="input"><div class="prompt input_prompt">In&nbsp;[35]:</div><div class="inner_cell">    <div class="input_area"><div class=" highlight hl-ipython3"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">hsplit</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span></pre></div>    </div></div></div><div class="output_wrapper"><div class="output"><div class="output_area">    <div class="prompt output_prompt">Out[35]:</div><div class="output_text output_subarea output_execute_result"><pre>[array([[6., 7., 6., 9.],        [8., 5., 5., 7.]]), array([[0., 5., 4., 0.],        [1., 8., 6., 7.]]), array([[6., 8., 5., 2.],        [1., 8., 1., 0.]])]</pre></div></div></div></div></div><div class="cell border-box-sizing code_cell rendered"><div class="input"><div class="prompt input_prompt">In&nbsp;[37]:</div><div class="inner_cell">    <div class="input_area"><div class=" highlight hl-ipython3"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">hsplit</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span></pre></div>    </div></div></div><div class="output_wrapper"><div class="output"><div class="output_area">    <div class="prompt output_prompt">Out[37]:</div><div class="output_text output_subarea output_execute_result"><pre>[array([[6., 7., 6.],        [8., 5., 5.]]), array([[9.],        [7.]]), array([[0., 5., 4., 0., 6., 8., 5., 2.],        [1., 8., 6., 7., 1., 8., 1., 0.]])]</pre></div></div></div></div></div><div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt"></div><div class="inner_cell"><div class="text_cell_render border-box-sizing rendered_html"><p><strong>数组复制</strong></p><p>常规赋值是浅复制,相当于复制了指针</p><p>函数传参也是浅复制</p></div></div></div><div class="cell border-box-sizing code_cell rendered"><div class="input"><div class="prompt input_prompt">In&nbsp;[40]:</div><div class="inner_cell">    <div class="input_area"><div class=" highlight hl-ipython3"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">6.</span><span class="p">,</span> <span class="mf">7.</span><span class="p">,</span> <span class="mf">6.</span><span class="p">,</span> <span class="mf">9.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">5.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">6.</span><span class="p">,</span> <span class="mf">8.</span><span class="p">,</span> <span class="mf">5.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">],</span>       <span class="p">[</span><span class="mf">8.</span><span class="p">,</span> <span class="mf">5.</span><span class="p">,</span> <span class="mf">5.</span><span class="p">,</span> <span class="mf">7.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">8.</span><span class="p">,</span> <span class="mf">6.</span><span class="p">,</span> <span class="mf">7.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">8.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">]])</span><span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="nb">print</span><span class="p">(</span><span class="n">b</span> <span class="ow">is</span> <span class="n">a</span><span class="p">)</span><span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span><span class="nb">print</span><span class="p">(</span><span class="n">b</span><span class="p">)</span><span class="n">a</span><span class="o">.</span><span class="n">resize</span><span class="p">((</span><span class="mi">4</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span><span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span><span class="nb">print</span><span class="p">(</span><span class="n">b</span><span class="p">)</span></pre></div>    </div></div></div><div class="output_wrapper"><div class="output"><div class="output_area">    <div class="prompt"></div><div class="output_subarea output_stream output_stdout output_text"><pre>True[[6. 7. 6. 9. 0. 5. 4. 0. 6. 8. 5. 2.] [8. 5. 5. 7. 1. 8. 6. 7. 1. 8. 1. 0.]][[6. 7. 6. 9. 0. 5. 4. 0. 6. 8. 5. 2.] [8. 5. 5. 7. 1. 8. 6. 7. 1. 8. 1. 0.]]None[[6. 7. 6. 9. 0. 5.] [4. 0. 6. 8. 5. 2.] [8. 5. 5. 7. 1. 8.] [6. 7. 1. 8. 1. 0.]][[6. 7. 6. 9. 0. 5.] [4. 0. 6. 8. 5. 2.] [8. 5. 5. 7. 1. 8.] [6. 7. 1. 8. 1. 0.]]</pre></div></div></div></div></div><div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt"></div><div class="inner_cell"><div class="text_cell_render border-box-sizing rendered_html"><p><strong>使用view方法创建一个新的数组，可以共享相同的数据</strong></p><ul><li>如果直接更改数据内容，源数组同步改变</li><li>修改形状不影响原始数据</li><li>直接赋值影响原始数据</li><li>切片操作返回的是一个view产生的对象</li></ul></div></div></div><div class="cell border-box-sizing code_cell rendered"><div class="input"><div class="prompt input_prompt">In&nbsp;[46]:</div><div class="inner_cell">    <div class="input_area"><div class=" highlight hl-ipython3"><pre><span></span><span class="n">c</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">view</span><span class="p">()</span><span class="nb">print</span><span class="p">(</span><span class="n">c</span> <span class="ow">is</span> <span class="n">a</span><span class="p">)</span><span class="nb">print</span><span class="p">(</span><span class="n">c</span><span class="o">.</span><span class="n">base</span> <span class="ow">is</span> <span class="n">a</span><span class="p">)</span></pre></div>    </div></div></div><div class="output_wrapper"><div class="output"><div class="output_area">    <div class="prompt"></div><div class="output_subarea output_stream output_stdout output_text"><pre>FalseTrue</pre></div></div></div></div></div><div class="cell border-box-sizing code_cell rendered"><div class="input"><div class="prompt input_prompt">In&nbsp;[47]:</div><div class="inner_cell">    <div class="input_area"><div class=" highlight hl-ipython3"><pre><span></span><span class="n">c</span><span class="o">.</span><span class="n">flags</span><span class="o">.</span><span class="n">owndata</span></pre></div>    </div></div></div><div class="output_wrapper"><div class="output"><div class="output_area">    <div class="prompt output_prompt">Out[47]:</div><div class="output_text output_subarea output_execute_result"><pre>False</pre></div></div></div></div></div><div class="cell border-box-sizing code_cell rendered"><div class="input"><div class="prompt input_prompt">In&nbsp;[55]:</div><div class="inner_cell">    <div class="input_area"><div class=" highlight hl-ipython3"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">6.</span><span class="p">,</span> <span class="mf">7.</span><span class="p">,</span> <span class="mf">6.</span><span class="p">,</span> <span class="mf">9.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">5.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">6.</span><span class="p">,</span> <span class="mf">8.</span><span class="p">,</span> <span class="mf">5.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">],</span>       <span class="p">[</span><span class="mf">8.</span><span class="p">,</span> <span class="mf">5.</span><span class="p">,</span> <span class="mf">5.</span><span class="p">,</span> <span class="mf">7.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">8.</span><span class="p">,</span> <span class="mf">6.</span><span class="p">,</span> <span class="mf">7.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">8.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">]])</span><span class="n">c</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">view</span><span class="p">()</span><span class="n">c</span> <span class="o">=</span> <span class="n">c</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">4</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span><span class="nb">print</span><span class="p">(</span><span class="n">c</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="n">c</span><span class="o">.</span><span class="n">resize</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">6</span><span class="p">)</span><span class="nb">print</span><span class="p">(</span><span class="n">c</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span></pre></div>    </div></div></div><div class="output_wrapper"><div class="output"><div class="output_area">    <div class="prompt"></div><div class="output_subarea output_stream output_stdout output_text"><pre>(4, 6) (2, 12)(4, 6) (2, 12)</pre></div></div></div></div></div><div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt"></div><div class="inner_cell"><div class="text_cell_render border-box-sizing rendered_html"><p><strong>深度复制</strong></p><ul><li>copy</li></ul><p>如果在切片之后不需要中间数据了，可以直接使用copy进行深度复制，然后再内存中直接删除中间数据</p></div></div></div><div class="cell border-box-sizing code_cell rendered"><div class="input"><div class="prompt input_prompt">In&nbsp;[58]:</div><div class="inner_cell">    <div class="input_area"><div class=" highlight hl-ipython3"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="mf">1e8</span><span class="p">))</span><span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="p">[:</span><span class="mi">100</span><span class="p">]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span><span class="k">del</span> <span class="n">a</span>  <span class="c1"># the memory of ``a`` can be released.</span><span class="nb">print</span><span class="p">(</span><span class="n">b</span><span class="p">)</span></pre></div>    </div></div></div><div class="output_wrapper"><div class="output"><div class="output_area">    <div class="prompt"></div><div class="output_subarea output_stream output_stdout output_text"><pre>[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99]</pre></div></div></div></div></div><div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt"></div><div class="inner_cell"><div class="text_cell_render border-box-sizing rendered_html"><p>Here is a list of some useful NumPy functions and methods names ordered in categories. See Routines for the full list.</p><p><strong>Array Creation</strong></p><p><font color="blue">arange, array, copy, empty, empty_like, eye, fromfile, fromfunction, identity, linspace, logspace, mgrid, ogrid, ones, ones_like, r_, zeros, zeros_like</font></p><p><strong>Conversions</strong></p><p><font color="blue">ndarray.astype, atleast_1d, atleast_2d, atleast_3d, mat</font></p><p><strong>Manipulations</strong></p><p><font color="blue">array_split, column_stack, concatenate, diagonal, dsplit, dstack, hsplit, hstack, ndarray.item, newaxis, ravel, repeat, reshape, resize, squeeze, swapaxes, take, transpose, vsplit, vstack</font></p><p><strong>Questions</strong></p><p><font color="blue">all, any, nonzero, where</font></p><p><strong>Ordering</strong></p><p><font color="blue">argmax, argmin, argsort, max, min, ptp, searchsorted, sort</font></p><p><strong>Operations</strong></p><p><font color="blue">choose, compress, cumprod, cumsum, inner, ndarray.fill, imag, prod, put, putmask, real, sum</font></p><p><strong>Basic Statistics</strong></p><p><font color="blue">cov, mean, std, var</font></p><p><strong>Basic Linear Algebra</strong></p><p><font color="blue">cross, dot, outer, linalg.svd, vdot</font></p></div></div></div><div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt"></div><div class="inner_cell"><div class="text_cell_render border-box-sizing rendered_html"><h2 id="深入理解">深入理解<a class="anchor-link" href="#深入理解">¶</a></h2><ul><li>广播机制(broadcasting rules)</li><li>高级索引机制及技巧</li></ul></div></div></div><div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt"></div><div class="inner_cell"><div class="text_cell_render border-box-sizing rendered_html"><p><strong>广播机制</strong></p><p>允许普通函数处理形状不同的参数</p><ul><li>如果输入的数组形状不同，在形状最小的数组的维度+1，直到所有数组的形状都相同</li><li>一个数组在特定维度的大小为1，则当作它在这个维度有最大维度来处理</li><li>应用广播规则之后，所有数组的形状都应该一致</li></ul></div></div></div><div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt"></div><div class="inner_cell"><div class="text_cell_render border-box-sizing rendered_html"><p><strong>高级索引-整数索引数组</strong></p><ul><li>numpy数组不仅可以通过常规的索引方法访问，还可以通过证书数组或者布尔数组来索引</li><li>如果用一个1维数组索引一个多维数组，默认多维数组的第一个维度</li><li>如果用多维数组索引多维数组，二者必须具有相同形状</li><li>a[i,j] = a[(i.j)]</li><li>可以借助 argmax使用</li><li>可以用来赋值</li><li>如果索引数列中有重复元素，则操作也重复，最终结果以最后一次操作为准</li></ul></div></div></div><div class="cell border-box-sizing code_cell rendered"><div class="input"><div class="prompt input_prompt">In&nbsp;[66]:</div><div class="inner_cell">    <div class="input_area"><div class=" highlight hl-ipython3"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">12</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="n">i</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span> <span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span><span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">])</span></pre></div>    </div></div></div><div class="output_wrapper"><div class="output"><div class="output_area">    <div class="prompt"></div><div class="output_subarea output_stream output_stdout output_text"><pre>[  0   1   4   9  16  25  36  49  64  81 100 121][ 1  1  9 64 25]</pre></div></div></div></div></div><div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt"></div><div class="inner_cell"><div class="text_cell_render border-box-sizing rendered_html"><p><strong>高级索引-布尔值索引</strong></p><ul><li>对每个位置的元素进行选取：形状需要和目标数组完全一致，True表示需要该位置元素，False表示i不需要该位置的元素(返回一个选中的元素的列表-1维）</li><li>生成Mandelbort set的图片</li><li>选取某个维度的数据</li></ul></div></div></div><div class="cell border-box-sizing code_cell rendered"><div class="input"><div class="prompt input_prompt">In&nbsp;[68]:</div><div class="inner_cell">    <div class="input_area"><div class=" highlight hl-ipython3"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">12</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span><span class="n">b</span> <span class="o">=</span> <span class="n">a</span> <span class="o">&gt;</span> <span class="mi">4</span><span class="nb">print</span><span class="p">(</span><span class="n">b</span><span class="p">)</span><span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="p">[</span><span class="n">b</span><span class="p">])</span></pre></div>    </div></div></div><div class="output_wrapper"><div class="output"><div class="output_area">    <div class="prompt"></div><div class="output_subarea output_stream output_stdout output_text"><pre>[[False False False False] [False  True  True  True] [ True  True  True  True]][ 5  6  7  8  9 10 11]</pre></div></div></div></div></div><div class="cell border-box-sizing code_cell rendered"><div class="input"><div class="prompt input_prompt">In&nbsp;[74]:</div><div class="inner_cell">    <div class="input_area"><div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span><span class="k">def</span> <span class="nf">mandelbrot</span><span class="p">(</span> <span class="n">h</span><span class="p">,</span><span class="n">w</span><span class="p">,</span> <span class="n">maxit</span><span class="o">=</span><span class="mi">20</span> <span class="p">):</span>    <span class="n">y</span><span class="p">,</span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ogrid</span><span class="p">[</span> <span class="o">-</span><span class="mf">1.4</span><span class="p">:</span><span class="mf">1.4</span><span class="p">:</span><span class="n">h</span><span class="o">*</span><span class="mi">1</span><span class="n">j</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">:</span><span class="mf">0.8</span><span class="p">:</span><span class="n">w</span><span class="o">*</span><span class="mi">1</span><span class="n">j</span> <span class="p">]</span>    <span class="n">c</span> <span class="o">=</span> <span class="n">x</span><span class="o">+</span><span class="n">y</span><span class="o">*</span><span class="mi">1</span><span class="n">j</span>    <span class="n">z</span> <span class="o">=</span> <span class="n">c</span>    <span class="n">divtime</span> <span class="o">=</span> <span class="n">maxit</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">z</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">maxit</span><span class="p">):</span>        <span class="n">z</span> <span class="o">=</span> <span class="n">z</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">c</span>        <span class="n">diverge</span> <span class="o">=</span> <span class="n">z</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">conj</span><span class="p">(</span><span class="n">z</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="o">**</span><span class="mi">2</span>            <span class="c1"># who is diverging</span>        <span class="n">div_now</span> <span class="o">=</span> <span class="n">diverge</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">divtime</span><span class="o">==</span><span class="n">maxit</span><span class="p">)</span>  <span class="c1"># who is diverging now</span>        <span class="n">divtime</span><span class="p">[</span><span class="n">div_now</span><span class="p">]</span> <span class="o">=</span> <span class="n">i</span>                  <span class="c1"># note when</span>        <span class="n">z</span><span class="p">[</span><span class="n">diverge</span><span class="p">]</span> <span class="o">=</span> <span class="mi">2</span>                        <span class="c1"># avoid diverging too much</span>    <span class="k">return</span> <span class="n">divtime</span><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">mandelbrot</span><span class="p">(</span><span class="mi">400</span><span class="p">,</span><span class="mi">400</span><span class="p">))</span></pre></div>    </div></div></div><div class="output_wrapper"><div class="output"><div class="output_area">    <div class="prompt output_prompt">Out[74]:</div><div class="output_text output_subarea output_execute_result"><pre>&lt;matplotlib.image.AxesImage at 0x27f95653b88&gt;</pre></div></div><div class="output_area">    <div class="prompt"></div><div class="output_png output_subarea "><img src="data:image/png;base64,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"></div></div></div></div></div><div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt"></div><div class="inner_cell"><div class="text_cell_render border-box-sizing rendered_html"><p><strong>ix_(): 组合不同的向量,对数组的每个元素进行操作</strong></p><p>使用ix_()函数组合不同的向量，使得所有向量能够直接进行运算。</p></div></div></div><div class="cell border-box-sizing code_cell rendered"><div class="input"><div class="prompt input_prompt">In&nbsp;[79]:</div><div class="inner_cell">    <div class="input_area"><div class=" highlight hl-ipython3"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">])</span><span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">8</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">4</span><span class="p">])</span><span class="n">c</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">8</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span><span class="n">ax</span><span class="p">,</span><span class="n">bx</span><span class="p">,</span><span class="n">cx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ix_</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">,</span><span class="n">c</span><span class="p">)</span><span class="nb">print</span><span class="p">(</span><span class="n">ax</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">bx</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">cx</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="n">result</span> <span class="o">=</span> <span class="n">ax</span><span class="o">+</span><span class="n">bx</span><span class="o">*</span><span class="n">cx</span><span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span><span class="n">result</span> <span class="o">=</span> <span class="n">ax</span><span class="o">+</span><span class="n">bx</span><span class="o">+</span><span class="n">cx</span><span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span><span class="nb">print</span><span class="p">(</span><span class="n">ax</span><span class="o">*</span><span class="n">bx</span><span class="o">*</span><span class="n">cx</span><span class="p">)</span></pre></div>    </div></div></div><div class="output_wrapper"><div class="output"><div class="output_area">    <div class="prompt"></div><div class="output_subarea output_stream output_stdout output_text"><pre>(4, 1, 1) (1, 3, 1) (1, 1, 5)[[[42 34 50 66 26]  [27 22 32 42 17]  [22 18 26 34 14]] [[43 35 51 67 27]  [28 23 33 43 18]  [23 19 27 35 15]] [[44 36 52 68 28]  [29 24 34 44 19]  [24 20 28 36 16]] [[45 37 53 69 29]  [30 25 35 45 20]  [25 21 29 37 17]]][[[15 14 16 18 13]  [12 11 13 15 10]  [11 10 12 14  9]] [[16 15 17 19 14]  [13 12 14 16 11]  [12 11 13 15 10]] [[17 16 18 20 15]  [14 13 15 17 12]  [13 12 14 16 11]] [[18 17 19 21 16]  [15 14 16 18 13]  [14 13 15 17 12]]][[[ 80  64  96 128  48]  [ 50  40  60  80  30]  [ 40  32  48  64  24]] [[120  96 144 192  72]  [ 75  60  90 120  45]  [ 60  48  72  96  36]] [[160 128 192 256  96]  [100  80 120 160  60]  [ 80  64  96 128  48]] [[200 160 240 320 120]  [125 100 150 200  75]  [100  80 120 160  60]]]</pre></div></div></div></div></div><div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt"></div><div class="inner_cell"><div class="text_cell_render border-box-sizing rendered_html"><h2 id="线性代数">线性代数<a class="anchor-link" href="#线性代数">¶</a></h2></div></div></div><div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt"></div><div class="inner_cell"><div class="text_cell_render border-box-sizing rendered_html"><h2 id="基本数组操作">基本数组操作<a class="anchor-link" href="#基本数组操作">¶</a></h2><ul><li>np.tanspose() 转置</li><li>np.linalg.inv() 求逆矩阵</li><li>np.eye(dimention) 单位矩阵</li><li><code>@</code> 矩阵乘法：<code>j @ j</code></li><li>np.trace 对角线元素求和(矩阵的迹）</li><li>np.linalg.solve(a, b) 求解线性矩阵方程AX=B，返回X</li><li>np.linalg.eig(j) 求j的特征向量</li><li>np.linalg.dot 两个数组的点积</li><li>np.linalg.vdot 两个向量的点积</li><li>np.linalg.inner 俩ing个数组的内积</li><li>np.linalg.matmul 两个数组的矩阵积</li><li>np.linalg.determinant 数组的行列式</li></ul></div></div></div><div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt"></div><div class="inner_cell"><div class="text_cell_render border-box-sizing rendered_html"><h2 id="直方图-Histograms">直方图 Histograms<a class="anchor-link" href="#直方图-Histograms">¶</a></h2><ul><li>numpy.histgram()： 生成直方图的数据（每个方块的高度）</li><li>matplotlib.plot.hist() 自动画出直方图</li></ul></div></div></div><div class="cell border-box-sizing code_cell rendered"><div class="input"><div class="prompt input_prompt">In&nbsp;[87]:</div><div class="inner_cell">    <div class="input_area"><div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span><span class="mf">0.5</span><span class="n">v</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="mi">10000</span><span class="p">)</span><span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">bins</span><span class="p">)</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">histogram</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="nb">print</span><span class="p">(</span><span class="s1">'n</span><span class="se">\n</span><span class="s1">'</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span><span class="nb">print</span><span class="p">(</span><span class="s1">'bins</span><span class="se">\n</span><span class="s1">'</span><span class="p">,</span> <span class="n">bins</span><span class="p">)</span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="o">.</span><span class="mi">5</span><span class="o">*</span><span class="p">(</span><span class="n">bins</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span><span class="o">+</span><span class="n">bins</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="n">n</span><span class="p">)</span></pre></div>    </div></div></div><div class="output_wrapper"><div class="output"><div class="output_area">    <div class="prompt"></div><div class="output_subarea output_stream output_stdout output_text"><pre>n [0.00118758 0.         0.         0.         0.         0.00118758 0.00237515 0.00593788 0.0047503  0.00950061 0.01662606 0.0344397 0.04987819 0.0593788  0.0890682  0.10806942 0.13775882 0.2030755 0.26126673 0.37883675 0.46671738 0.47978072 0.56528619 0.6294153 0.71017047 0.78736291 0.82892807 0.82417777 0.73392199 0.72085865 0.67691834 0.59616317 0.55578558 0.46790496 0.36933615 0.3218331 0.20782581 0.18051156 0.12350791 0.0843179  0.06650426 0.04037759 0.01900122 0.01900122 0.01781364 0.01187576 0.00356273 0.00118758 0.         0.00237515]bins [-0.28356776 -0.19936262 -0.11515749 -0.03095235  0.05325278  0.13745792  0.22166305  0.30586819  0.39007332  0.47427846  0.55848359  0.64268873  0.72689386  0.811099    0.89530413  0.97950927  1.0637144   1.14791954  1.23212467  1.31632981  1.40053494  1.48474008  1.56894521  1.65315035  1.73735548  1.82156062  1.90576575  1.98997089  2.07417602  2.15838116  2.24258629  2.32679143  2.41099656  2.4952017   2.57940683  2.66361197  2.7478171   2.83202224  2.91622737  3.00043251  3.08463764  3.16884278  3.25304791  3.33725305  3.42145818  3.50566332  3.58986845  3.67407359  3.75827872  3.84248386  3.92668899]</pre></div></div><div class="output_area">    <div class="prompt output_prompt">Out[87]:</div><div class="output_text output_subarea output_execute_result"><pre>[&lt;matplotlib.lines.Line2D at 0x27f9afed488&gt;]</pre></div></div><div class="output_area">    <div class="prompt"></div><div class="output_png output_subarea "><img src="data:image/png;base64,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"></div></div></div></div></div>    </div>  </div>        <script src="//cdn.jsdelivr.net/npm/js-base64/base64.min.js"></script>        <script>        const hasAttr = (e,a) => a.some(_=> e.attr(_)!==undefined);        $('a').each(function() {          const $this = $(this);          if(hasAttr($this,["data-fancybox","ignore-external-link"])) return;          const href = $this.attr('href');          if (href && href.match('^((http|https|thunder|qqdl|ed2k|Flashget|qbrowser|ftp|rtsp|mms)://)')) {            const strs = href.split('/');            if (strs.length >= 3) {                const host = strs[2];                if (host !== 'your_domain' || window.location.host) {                    $this.attr('href', '/go.html?u='+Base64.encode(href)+'').attr('rel', 'external nofollow noopener noreferrer');                    if (true) {                        $this.attr('target', '_blank');                    }                }            }          }        });        </script>]]></content>
      
      
      <categories>
          
          <category> 库 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>算法学习笔记</title>
      <link href="/2020/03/06/other/%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
      <url>/2020/03/06/other/%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="刷题笔记"><a href="#刷题笔记" class="headerlink" title="刷题笔记"></a>刷题笔记</h1><h2 id="Sample-Level"><a href="#Sample-Level" class="headerlink" title="Sample Level"></a>Sample Level</h2><ul><li><p>排序</p><ul><li><p>B1015/A1062:</p><p>  这是一道分类排序题，可以将成绩分成5个类别，最后一个类别不用输出。</p><p>  思路一：将类别作为结构体的一个成员，将所有学生进行排序（快速排序算法），平均复杂度为：$O(NlogN)$。<br>  思路二：分类存储每一类成绩，再分别对其排序。</p><p>  <strong>超时问题</strong>：</p><p>  最开始以为是因为排序算法的复杂度过高导致的超时，后经查阅其他人的博客发现，超时的原因是<strong>cin的速度比较慢，这里应该使用scanf函数来控制输入</strong>，如果使用cin函数，应该使用以下函数<code>ios::sync_with_stdio(false)</code>以取消<code>cin</code>和<code>cout</code>的同步机制（但是这种方法还是比<code>scanf</code>慢，因此此题建议使用<code>scanf</code></p><p>  总结：</p><ol><li><p>基本排序算法的实现-快速排序</p><p> 快速排序每次选择一个哨兵<code>i</code>作为标准，将整个序列分成两个部分：大于<code>i</code>的元素和小于<code>i</code>的元素。通过递归每次将两个部分再次进行排序，最终完成对序列整体的排序。</p><p> 代码实现：</p><pre class=" language-lang-(C++)"><code class="language-lang-(C++)"> // 函数中的compare函数自己实现 void quick_sort(Student students[], int start, int ends){     if(start < ends){         int low = start, high = ends;         Student tmp = students[low];         while(low < high){//把数组分成两个部分：应该排在tmp前面的和后面的             while(low < high && compare(tmp, students[high]) == true){                 //如果tudents[high]排在tmo的后面                 high--;             }             students[low] = students[high];             while(low < high && compare(tmp, students[low]) == false){                 low++;             }             students[high] = students[low];         }         students[low] = tmp;         quick_sort(students, start, low-1);         quick_sort(students, low+1, ends);     } }</code></pre></li><li><p>使用库函数：<code>sort()</code>函数的使用方法</p><p> 下面简单介绍sort()函数的使用方法：</p><p> <code>sort()</code>函数包含在<code>algorithm</code>库中</p><p> <strong>Sorting:</strong><br> | method name | function |<br> | :—————-:|:—————:|<br> | sort |Sort elements in range (function template ) |<br> | stable_sort | Sort elements preserving order of equivalents (function template ) |<br> | partial_sort | Partially sort elements in range (function template )|<br> |partial_sort_copy|Copy and partially sort range (function template )|<br> |is_sorted | Check whether range is sorted (function template )|<br> | is_sorted_until| Find first unsorted element in range (function template )|<br> | nth_element|Sort element in range (function template )|</p><p> <code>sort</code>的功能是对一个区间内的元素进行排序，默认非降序排序，可以通过自己实现compare函数控制排序。（不稳定排序，如果需要使用稳定排序，可以使用<code>stable_sort</code>函数进行排序）</p><p> <code>sort(first, last, comp)</code>对$[first, last)$范围内的元素进行排序（左开右闭）</p><p> <code>comp</code>参数是一个函数指针，这个函数应该返回一个<code>bool</code>值，sort函数根据bool值进行排序。</p><p> e.g.,</p><p> <code>bool comp(Elem a, Elem b)</code>返回<code>true</code>, 则表示排序过程中，<code>a</code>应该排在<code>b</code>的<strong>前面</strong>。</p><p> 实例1，vector排序：</p><pre class=" language-lang-(C++)"><code class="language-lang-(C++)"> bool compare(Stu &s1, Stu &s2){     int flag = false;     if(s1.sum != s2.sum) return s1.sum > s2.sum;     else if(s1.de != s2.de) return s1.de > s2.de;     else return s1.id < s2.id;     return flag; } //省略其他代码， stus是一个vector数组 for(int i = 0; i < 4; i++){     sort(stus[i].begin(), stus[i].end(), compare); }</code></pre><p> 实例2，数组排序：</p><pre class=" language-lang-(C++)"><code class="language-lang-(C++)"> //students是长度为m的数组 sort(students, students + m, compare);</code></pre></li><li><p><code>algorithm</code>库中可以使用的其他排序算法（见上表）</p></li><li>涉及到排序问题的关键在于怎样实现compare函数，通常根据题目的条件来实现相应的compare函数</li></ol></li><li><p>子问题：排名(Rank)</p><p>排名问题的要点在于：如果存在相同分数的记录，排名应该保持相同，直到下一个分数出现才累计排名。</p><p>e.g., 1，2，2，4，5，5，5，8</p><ul><li><p>A1012</p><ul><li>简单的模拟题，最开始被我弄得很复杂，最终参考题解的代码如下：</li></ul><pre class=" language-lang-(C++)"><code class="language-lang-(C++)">#include<iostream>#include<stdlib.h>#include<stdio.h>#include<math.h>#include<algorithm>using namespace std;const int maxn=2010;struct student{    int id;    int grade[4];};char course[4]={'A','C','M','E'};student stu[maxn];int RANK[10000000][4]={0};int now;//当前第几门课bool cmp(student a,student b){    return a.grade[now]>b.grade[now];}int main(){    int N,M;    scanf("%d%d",&N,&M);    for(int i=0;i<N;i++){        scanf("%d%d%d%d",&stu[i].id,&stu[i].grade[1],&stu[i].grade[2],&stu[i].grade[3]);        stu[i].grade[0]=round((stu[i].grade[1]+stu[i].grade[2]+stu[i].grade[3])/3.0)+0.5;    }    for(now=0;now<4;now++){        sort(stu,stu+N,cmp);        RANK[stu[0].id][now]=1;        for(int i=1;i<N;i++){            if(stu[i].grade[now]==stu[i-1].grade[now]){                RANK[stu[i].id][now]=RANK[stu[i-1].id][now];            }else{                RANK[stu[i].id][now]=i+1;            }        }    }    int query;    for(int i=0;i<M;i++){        scanf("%d",&query);        if(RANK[query][0]==0){            printf("N/A\n");        }else{            int k=0;            for(int j=0;j<4;j++){                if(RANK[query][j]<RANK[query][k]){                    k=j;                }            }            printf("%d %c\n",RANK[query][k],course[k]);        }    }    system("pause");    return 0;}</code></pre></li><li><p>A1025</p></li></ul><p>题目是合并几块无序数据，并且按照分数进行总的排序。最终输出总排名和局部排名。</p></li></ul></li><li><p>简单Hash问题</p><ul><li>常用思路:<ul><li>ASCII码散列，处理字符串相减等问题</li><li>检查某些项是否出现</li><li>两数相加问题</li></ul></li><li>简单问题中，可以通过使用散列的方法，降低时间复杂度。重点是在这个过程中，应该要考虑好散列表的大小，确保数据完全散列到表内。</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 笔记 </tag>
            
            <tag> 算法 </tag>
            
            <tag> 刷题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Word2Vec源码理解</title>
      <link href="/2020/03/06/other/word2vec%E6%BA%90%E7%A0%81%E7%90%86%E8%A7%A3/"/>
      <url>/2020/03/06/other/word2vec%E6%BA%90%E7%A0%81%E7%90%86%E8%A7%A3/</url>
      
        <content type="html"><![CDATA[<a id="more"></a><h1 id="Word2Vec源码学习"><a href="#Word2Vec源码学习" class="headerlink" title="Word2Vec源码学习"></a>Word2Vec源码学习</h1><h2 id="理解Word2Vec模型"><a href="#理解Word2Vec模型" class="headerlink" title="理解Word2Vec模型"></a>理解Word2Vec模型</h2><h2 id="具体实现"><a href="#具体实现" class="headerlink" title="具体实现"></a>具体实现</h2><h2 id="Google源码分析"><a href="#Google源码分析" class="headerlink" title="Google源码分析"></a>Google源码分析</h2>]]></content>
      
      
      <categories>
          
          <category> 源码分析 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> 笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>统计学习方法学习笔记</title>
      <link href="/2020/03/04/other/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"/>
      <url>/2020/03/04/other/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<h1 id="统计学习方法学习笔记"><a href="#统计学习方法学习笔记" class="headerlink" title="统计学习方法学习笔记"></a>统计学习方法学习笔记</h1><h2 id="概论"><a href="#概论" class="headerlink" title="概论"></a>概论</h2><p>本部分主要讲述了：</p><ul><li>统计学习的研究内容</li><li>统计学习的三要素</li><li>监督学习的学习方法</li><li>对于模型的评价</li><li>监督学习的三个子问题</li></ul><ol><li>统计学习<ul><li>假设数据有统计规律</li><li>通过对数据进行分析，发现其中的知识</li><li>学习： 一个系统通过执行某个过程来改进它的性能</li><li>目的是对数据进行预测和分析</li><li>主要方法<ul><li>监督学习(supervised learning)</li><li>非监督学习(insupervised learning)</li><li>半监督学习(semi-supervised learning)</li><li>强化学习(reinforcement learning)</li></ul></li></ul></li><li>监督学习<ul><li>任务：学习一个模型。该模型能够对输出产生较好的预测结果</li><li>基本概念：<ol><li>输入空间，特征空间，输出空间</li><li>联合概率分布，条件概率（决策函数）</li><li>假设空间</li></ol></li></ul></li><li><p>统计学习三要素</p><ul><li>模型：模型的假设空间，即需要学习的函数属于的某一个函数空间<ol><li>通过模型学习输入和输出的联合概率密度（生成方法）或者条件概率密度（判别方法，也叫决策函数）</li></ol></li><li><p>策略：模型选择的准则，用于评价假设空间中的模型，以选择一个最优模型</p><ol><li>损失函数(loss function)/代价函数(cost function):<blockquote><ul><li>衡量模型一次预测的好坏</li><li>wait to insert LaTex function<ul><li>0-1(0-1 loss function)</li><li>平方损失(quadrtic loss function)</li><li>绝对损失(absolute loss function)</li><li>对数损失(logarithmic loss function)</li></ul></li></ul></blockquote></li><li><p>风险函数(risk function)</p><blockquote><ul><li>平均意义下的模型的好坏，</li></ul></blockquote><p> 损失函数的期望,即风险函数(risk function)或者是期望损失(expected loss)</p><p> 因为风险函数中的联合概率分布未知，因此使用经验风险估计期望风险。(当样本数量足够多的时候可行，实际中样本往往较少，因此需要对这个函数进行修正)</p></li><li>经验风险(empirivcal risk)/经验损失(empirical loss)<ul><li>训练集上的平均损失</li><li>矫正策略：<ul><li>经验风险最小化（容易引起过拟合(overfitting)）</li><li>结构风险最小化<ul><li>同时表示模型的经验风险和模型的复杂度</li><li>正则化</li></ul></li><li>将统计学习问题转化为最优化问题</li></ul></li></ul></li></ol></li><li>算法：模型学习的算法<ul><li>统计学习归结为最优化问题，因此可采用最优化算法来求解最优化模型</li></ul></li></ul></li><li>模型评估<ul><li>训练误差：训练数据集的平均损失<ul><li>可以判断该问题是否可以学习，但是本质上是不重要的</li></ul></li><li>测试误差：测试数据集的平均损失<ul><li>测试误差用于判断模型的泛化能力，决定模型的好坏</li></ul></li></ul></li><li>模型选择<ul><li>过拟合<ul><li>选择的模型复杂度比较高（比实际模型高）</li><li>包含参数过多</li></ul></li><li>模型选择<ul><li>复杂度适当</li></ul></li></ul></li><li>正则化和交叉验证<ul><li>正则化：<ul><li>在经验风险的基础上增加正则化项或者罚项。</li></ul></li><li>交叉验证<ul><li>在数据集不足的时候重复使用数据来对模型进行验证</li><li>简单交叉验证<ul><li>随机拆分数据为训练集和测试机</li><li>在不同的条件下训练模型，在测试集上测试这些模型，选择最好的</li></ul></li><li>S折交叉验证(S-fold cross validation)<ul><li>将数据集分成S个不相交子集，S-1个用于训练，1个用于测试</li><li>重复所有S种可能，选择平均测试误差最小的模型</li></ul></li><li>留一交叉验证(leave one out cross validation)<ul><li>S-fold的特殊情况，S=N</li></ul></li></ul></li></ul></li><li>泛化能力<ul><li>评价标准：泛化误差上界</li><li>泛化误差(generalization ability)</li><li>泛化误差上界：<ul><li>是N（样本容量）的函数，N增大，上界-&gt;0</li><li>是假设空间容量的函数，capacity越大，上界越大</li></ul></li><li>二分类问题的泛化误差上界</li></ul></li><li>生成模型和判别模型<ul><li>生成模型：学习的是X，Y的联合分布函数g(X,Y)<ul><li>朴素贝叶斯</li><li>隐马尔可夫</li></ul></li><li>判别模型：学习的是X,Y的条件概率分布（也即决策函数）<ul><li>K-近邻</li><li>感知机</li><li>决策树</li><li>逻辑斯蒂回归</li><li>最大熵</li><li>支持向量机</li><li>提升方法</li><li>条件随机场</li></ul></li></ul></li><li>分类问题（连续/离散输入 —&gt; 离散输出）<ul><li>二分类<ul><li>预测情况：<ul><li>TP: True Positive</li><li>FN: False Negetive</li><li>TN: True Negetive</li><li>FP: False Positive</li></ul></li><li>评价标准：<ul><li>精确率(precision)</li><li>召回率(recall)</li><li>F1值</li></ul></li></ul></li><li>多分类</li></ul></li><li>标注问题（序列输入 —&gt; 序列输出）<ul><li>分类问题的推广</li><li>结构预测的简单形式</li><li>输入：观测序列</li><li>输出：标记序列或者是状态序列</li></ul></li><li>回归问题<ul><li>预测输入变量和输出变量的关系</li><li>等价于函数拟合</li><li>分类<ul><li>按输入维度：<ul><li>一元回归</li><li>多元回归</li></ul></li><li>按模型类型<ul><li>线性</li><li>非线性</li></ul></li></ul></li></ul></li></ol><h2 id="感知机"><a href="#感知机" class="headerlink" title="感知机"></a>感知机</h2><blockquote><p>二类分类，线性模型</p></blockquote><h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><ul><li><p>特征空间中的线性分类模型的集合，属于判别模型（模型是一个输入和输出是线性关系）</p><script type="math/tex; mode=display">f(x) = sign(w\cdot{x}+b)</script></li><li><p>输入：n维特征向量 $x$</p></li><li>输出：$y=+1$或者$y=-1$</li><li>参数：<ul><li>$w$：超平面的法向量，</li><li>$b$：超平面的截距</li></ul></li></ul><h3 id="学习策略"><a href="#学习策略" class="headerlink" title="学习策略"></a>学习策略</h3><ul><li>损失函数极小化</li><li>误分类点</li><li><p>损失函数：误分类点到超平面的总距离</p><p>  $-\dfrac{1}{|w|}\displaystyle\sum_{x_i\in M}|w\cdot{x_i}+{b}|$</p></li><li>$|w|$是$w$的$L_2$范数</li></ul><h3 id="学习算法"><a href="#学习算法" class="headerlink" title="学习算法"></a>学习算法</h3><ul><li>随机梯度下降（SGD）</li><li>原始形式<ul><li>w表示为原始形式</li></ul></li><li>对偶形式<ul><li>w用$\displaystyle\sum_{i=1}^N{\alpha_iy_ix_i}$表示</li></ul></li></ul><h2 id="k-近邻"><a href="#k-近邻" class="headerlink" title="k-近邻"></a>k-近邻</h2><p>基本的回归和分类方法</p><blockquote><p>特征空间划分</p></blockquote><ul><li>原理<ul><li>根据k个<strong>最近邻</strong>的实例确定新实例的类别，例如通过多数表决确定</li></ul></li><li>三要素<ul><li>距离度量<ul><li>欧式距离($L_2$距离)</li><li>$L_p$距离：$L_p(x_i,x_j)=(\displaystyle\sum_{l=1}^n{|x_i^{(l)}-x_j^{(l)}|}^p)^\frac{1}{p}$</li><li>Minkowski距离</li></ul></li><li>k值的选择<ul><li>k小：近似误差小，估计误差大</li><li>k大：近似误差大，估计误差小</li></ul></li><li>分类决策规则<ul><li>多数表决</li></ul></li></ul></li><li>实现<ul><li>线性扫描</li><li>kd树（这里的k表示空间的维度k）</li></ul></li></ul><h2 id="朴素贝叶斯"><a href="#朴素贝叶斯" class="headerlink" title="朴素贝叶斯"></a>朴素贝叶斯</h2><p>基于贝叶斯定理、特征条件独立假设、分类方法</p><ul><li>学习的是联合概率分布</li></ul><h2 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h2><ul><li>基本分类和回归方法</li></ul><h2 id="逻辑斯蒂回归和最大熵"><a href="#逻辑斯蒂回归和最大熵" class="headerlink" title="逻辑斯蒂回归和最大熵"></a>逻辑斯蒂回归和最大熵</h2><h2 id="支持向量机"><a href="#支持向量机" class="headerlink" title="支持向量机"></a>支持向量机</h2><h2 id="提升方法"><a href="#提升方法" class="headerlink" title="提升方法"></a>提升方法</h2><h2 id="EM算法及其推广"><a href="#EM算法及其推广" class="headerlink" title="EM算法及其推广"></a>EM算法及其推广</h2><h2 id="隐马尔可夫模型"><a href="#隐马尔可夫模型" class="headerlink" title="隐马尔可夫模型"></a>隐马尔可夫模型</h2><h2 id="条件随机场"><a href="#条件随机场" class="headerlink" title="条件随机场"></a>条件随机场</h2><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2>]]></content>
      
      
      <categories>
          
          <category> 学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 笔记 </tag>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
